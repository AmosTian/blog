---
title: 5.信息论
categories:	
  - AI
  - 数学基础
tags:
  - AI
  - 数学基础
top: 5
mathjax: true
date: 2023-8-20 14:45:08
---

不确定性才是客观世界的本质属性。不确定性的世界只能使用概率模型来描述，正是对概率模型的刻画促成了信息论的的诞生

香农——通信的数学理论，给定了对信息这一定性概念的定量分析方法

信息论在世界的不确定性和消息的可测量性之间搭建桥梁

<!--more-->

## 5.1 概念

### 5.1.1 基本问题

在一点精确或近似地复现在另一点选取的消息

消息通常有意义，即根据某种体系，消息本身指向或关联着物理上或概念上的特定实体，但消息的语义与(产生/接收消息的)工程问题无关——将信息的传播与处理完全剥离

一条消息来自于一个所有可能的消息的集合——所有与类型的消息都被抽象为逻辑符号，拓展了通信任务的范畴与信息论的适用性

### 5.1.2 信息熵

信息论用 **信息熵** 的概念对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释

在生活中，信息的载体是消息，不同的消息带来的信息量也是不同的

- 消息1：中国男足夺世界杯冠军，赔率 $1:1000$
- 消息2：中国男乒夺世界杯冠军，赔率 $1:2$

不确定性越大(小概率事件发生的概率)的消息可能性越小，提供的信息量就越大

- **一个消息的不确定性(信息量)的表征——熵，一个系统内在的混乱程度**

若事件 $A$ 发生的概率为 $P(A)$ ，则这个事件的信息量为 $h(A)=-log_2P(A)$

- 消息1信息量 $h(A)=10$
- 消息2信息量 $h(A)=1$

### 5.1.3 信源的信息熵

**信源的信息熵** 是信源可能发出的各个符号的自信息量在信源构成的概率空间上的统计平均值

一个离散的信源 $X$ 包含 $m$ 个符号，每个符号 $a_i$ 的取值概率为 $P(a_i)$ ，则 $X$ 的信息熵为
$$
H(X)=-\sum_\limits{i=1}^mP(a_i)log_2P(a_i)
$$
信源的信息熵描述了每发送一个符号所提供的平均信息量，是信源总体信息测度的均值 

当信源中的每个符号的取值概率相等时，信息熵取到最大值 $-log_2m$ ，意味着信源的随机程度最高

### 5.1.4 条件熵

如果两个信源之间有相关性，在已知其中一个信源 $X$ 的条件下，另一个信源 $Y$ 的信息熵就会减小

用 **条件熵** $H(Y\vert X)$ 表示在已知随机变量 $X$ 的条件下另一个随机变量 $Y$ 的不确定性
$$
H(Y\vert X)=\sum_\limits{i=1}^mP(x_i)H(Y\vert X=x_i)=-\sum_\limits{i=1}^m\left[P(x_i)\sum_\limits{j=1}^pP(y_j\vert x_i)log_2P(y_j\vert x_i)\right]\\=-\sum_\limits{i=1}^m\sum_\limits{j=1}^pP(x_i,y_j)log_2P(y_i\vert x_i)
$$
条件熵的意义在于先按照变量 $X$ 的取值对变量 $Y$ 进行了一次分类，对每个分出来的类别计算其单独的信息熵，再将每个类的信息熵按照 $X$ 的分布计算数学期望——**分类会带来不确定性的下降**

- 教室中可能出现的座位分布在不加约束时，信息熵会很大
- 对座位的选择添加限制条件（如男左女右），相对于未加限制时的情形会简单很多

#### 互信息

$$
I(X;Y)=H(Y)-H(Y\vert X)
$$

可以看做 $X$ 给 $Y$ 带来的信息增益——机器学习领域

**信息增益常被用于分类特征的选择**。

- 对于给定的训练数据集 $Y$ ，$H(Y)$ 表示在未给定任何特征时，对训练集进行分类的不确定性
- $H(Y\vert X)$ 表示了使用特征 $X$ 对训练集 $Y$ 进行分类的不确定性
- 信息增益表示特征 $X$ 带来的对训练集 $Y$ 分类不确定性的减少程度，即特征 $X$ 对训练集 $Y$ 的区分度

信息增益更大的特征具有更强的分类能力。

---

由于其值很大程度上依赖于数据集的信息熵 $H(Y)$，不具有绝对意义，故有 **信息增益比** 的概念，$g(X,Y)=\frac{I(X;Y)}{H(Y)}$

### 5.1.5 KL散度

描述两个概率分布 $P$ 和 $Q$ 之间的差异的一种方法，定义为 $D_{KL}(P\Vert Q)=\sum_\limits{i=1}^np(x_i)log_2\frac{p(x_i)}{q(x_i)}$ 

**KL散度是对额外信息量的衡量** 

给定一个信源，其符号的概率分布为 $P(X)$ ，设计一种针对 $P(X)$ 的最优编码方式M，使得该信源所需的平均比特数最少（等于信息熵）

当信源的符号集合不变，但其概率分布变为 $Q(X)$ ，再用同样的编码方式M对编其编码，此时会比最优值多一些比特（信息熵会增加）

KL散度用于衡量这种情况下平均每个字符多用的比特数，也可以**表示两个分布之间的距离**

---

非负性：KL散度 $\ge 0$ ，只有在两个分布完全相同时取等

非对称性：$D_{KL}(P\Vert Q)\neq D_{KL}(Q\Vert P)$

- 要让 $D_{KL}(P\Vert Q)$ 最小，需要让 $Q(X)$ 在 $P(X)$ 不等于0的位置不等于0

- 要让 $D_{KL}(Q\Vert P)$ 最小，需要让 $Q(X)$ 在 $P(X)$ 等于0的位置等于0

## 5.2 最大熵原理

> 确定随机变量统计特性时，试图最符合客观情况的一种准则

对于一个未知的概率分布，最坏情况就是他等可能的取到每个取值，此时随机变量的随机程度最高，对其进行预测也最困难

- 得到最不确定的结果， 预测的风险越小

### 5.2.1 最大熵模型

首先确定若干特征函数作为分类依据

为保证特征函数的有效性，其在模型真实分布 $P(X)$ 上的数学期望和在由训练数据集上推导出的经验分布 $\widetilde{P}(X)$ 上的数学期望应该相等——对特征函数数学期望的估计应该是个无偏估计量

此时，每个特征函数就对应了一个约束条件。

**分类的任务就是在约束条件下，确定一个最好的模型**。

由于除这些约束条件外，没有任何关于分类的先验知识，因而需要利用最大熵原理，求解出不确定性最大的条件分布，即让以下函数取最大值
$$
H(P)=-\sum_\limits{x,y}\widetilde{P}(x)P(y\vert x)log_2P(y\vert x)
$$
$P(y\vert x)$ 为分类问题要确定的目标条件分布

---

上述计算过程实质上就是一个约束优化问题。从数学上可以证明，这个模型的解是存在且唯一的
