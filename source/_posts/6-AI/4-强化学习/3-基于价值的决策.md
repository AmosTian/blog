---
title: 3-基于价值的决策
categories:
  - AI
  - 机器学习
  - 强化学习
tags:
  - AI
  - 机器学习
  - 强化学习
mathjax: true
abbrlink: 1184554208
date: 2024-02-20 18:18:40
---

[TOC]

<!--more-->

## 3.0 策略的获取

### 3.0.1 价值函数的计算

- 动态规划法(dynamic programming, DP)——有模型学习, (Model-based Reinforcement Learning, MBRL)
- 蒙特卡洛方法(Monte Carlo, MC)——免模型学习(Model-free Reinforcement Learning, MFRL)
- 时序差分学习(temporal-difference learning, TD learning)，前两者的结合

> 强化学习中的 **模型** ：若环境的动态特性($P(s',r\vert s,a)$)已知，则认为环境已知。智能体对环境的建模称为模型

有模型学习常用的价值函数计算方法有

- 蒙特卡洛方法
- 动态规划方法
  - 价值迭代
  - 策略迭代
- 用数据估计出一个环境模型，进而基于动态规划方法计算价值函数

免模型学习的价值函数计算方法

- 蒙特卡洛方法：将策略迭代中，基于模型的部分替换为免模型部分

  更新时间：2024-2-24 10:28:04

- 时序差分方法

  更新时间：2024-2-27 23:34:53

### 3.0.2 最优策略的获取

 当获取最优价值函数后，可以通过取使 $V(s)$ 最大的动作来得到最佳策略
$$
\pi^*(a\vert s)=\begin{cases}
1,&a=\mathop{\mathrm{argmax}}\limits_{a}Q(s,a)\\
0,&其他
\end{cases}
$$
最简单的策略搜索办法是 **穷举** 。假设动作和状态都是有限的，则对于确定性策略空间有 $\vert \mathcal{A}\vert ^{\vert \mathcal{S}\vert}$ 个可能的策略，基于每个策略，算出状态价值函数，取最大状态价值对应的策略即可。

但穷举法效率低，可采取其他更好地最佳策略搜索方法

## 3.1 动态规划方法——有模型

- 策略迭代
- 价值迭代

- 截断策略迭代

### 3.1.1 价值迭代

#### 最优性原理

在策略 $\pi(a\vert s)$ 下，一个状态 $s$ 达到了最优价值( $V_\pi(s)=V^*(s)$ ) ，当且仅当对于任何能够从 $s$ 到达的状态 $s'$ ，都已经达到了最优价值，即对于所有的 $s'$ ，有 $V_\pi(s')=V^*(s')$ 恒成立——最优子结构

#### 确定性价值迭代

由贝尔曼最优方程可知
$$
\begin{aligned}
V^{(k+1)}(s)&=f(V^{(k)})=\max\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)}\right),k=0,1,2,3,\cdots\\
&=\max\limits_{\pi\in\Pi}\sum\limits_{a}\pi(a\vert s)Q(s,a)\\
&\xlongequal{Q=f(V)}\max\limits_{\pi\in\Pi}\sum\limits_{a}\pi(a\vert s)\left(R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V^*(s')\right)
\end{aligned}
$$
为了得到每个状态下的最优状态价值 $V^*$ ，可直接将贝尔曼最优方程作为更新规则进行迭代，迭代多次后，价值函数就会收敛，这种价值迭代算法也被称为确定性价值迭代

##### 迭代算法

初始化：令 $k=0$ ，对所有状态 $s$ ，$V^{(0)}(s)=0$ 

1. 第 $k$ 轮迭代，$k=0,1,2,\cdots$：

   计算  $Q^{(k)}$ 
   $$
   \mathbf{Q}^{(k)}(s,a)=\mathbf{R}(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot \mathbf{V}^{(k)}(s')\\
   $$
   ![image-20240123174813238](3-基于价值的决策/image-20240123174813238.png)

   策略更新
   $$
   \pi^{(k+1)}(s)=\mathop{\mathrm{argmax}}\limits_{\pi}\sum\limits_{a}\pi(a\vert s)Q^{(k)}(s,a)\\
   \Updownarrow\\
   \pi^{(k+1)}(a\vert s)=\begin{cases}
   1&a=a_{*}^{(k)}\\
   0&a\neq a_{*}^{(k)}
   \end{cases}\quad,a_{*}^{(k)}=\mathop{\mathrm{argmax}}\limits_a\mathbf{Q}^{(k)}(s,a)
   $$

2. 价值更新
   $$
   \begin{aligned}
   \mathbf{V}^{(k+1)}(s)&=\mathbf{R}_{\pi^{(k+1)}}(s,a)+\gamma \sum\limits_{s'\in S}P_{\pi^{(k+1)}}(s'\vert s,a)\cdot \mathbf{V}^{(k)}(s')\\
   &=\sum\limits_{a}\pi^{(k+1)}(a\vert s)\underbrace{\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'}P(s'\vert s,a)V^{(k)}(s')\right)}_{Q^{(k)}(s,a)}
   \end{aligned}
   $$
   由于采用确定性策略，所以最优动作的概率 $\pi(a^*\vert s)=1,\pi(a^{others}\vert s)=0$ 。相当于
   $$
   V^{(k+1)}=\max\limits_{a}Q^{(k)}(a,s)
   $$

在收敛后提取最优策略，$H$是让 $V(s)$ 收敛所需的迭代次数
$$
\pi^*(s)=\mathop{\mathrm{argmax}}_a\left[R(s,a)+\gamma\sum\limits_{s'\in S}P(s'\vert s,a)V^{(H+1)}(s')\right]
$$

###### 伪代码

$$
V^{(0)}(s),V^{(k)}(s)\rightarrow Q^{(k)}(s,a)\rightarrow 贪心策略 \pi^{(k+1)}(a\vert s)\rightarrow V^{(k+1)}=\max\limits_{a}Q^{(k)}(s,a)
$$

$$
\begin{array}{ll}
\hline
&初始化：已知P(r\vert s,a)和P(s'\vert s,a)，用随机值初始化V^{(0)}\\
&目标：求解贝尔曼最优方程进而得出最优状态价值与最优策略\\
&第k轮迭代,当\mathbf{V}^{(k)}未收敛时(\Vert V^{(k)}-V^{(k-1)}\Vert>\varepsilon)，进行：\\
&\quad对于 \forall s\in \mathcal{S}:\\
&\qquad 对于 \forall a\in \mathcal{A}(s):\\
&\quad \qquad 计算Q值：Q^{(k)}(s,a)=\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'}P(s'\vert s,a)V^{(k-1)}(s')\\
&\qquad 求最优动作：a_{*}^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q^{(k)}(s,a)\\
&\qquad 策略更新：\pi^{(k+1)}(a_{*}^{(k)}\vert s)=1,\pi^{(k+1)}(a^{others}\vert s)=0\\
&\qquad 价值更新：V^{(k+1)}=\max\limits_{a}Q^{(k)}(a,s)\\
\hline
\end{array}
$$

###### 价值迭代例子

条件：1个禁止区域 $s_2$ ，1个目标区域 $s_4$ 。奖励设置为 $r_{boundary}=r_{forbidden}=-1,r_{target}=1,r_{others}=0$ ，$a_1,a_2,a_3,a_4=上,右,下,左$ 

![image-20240220233518921](3-基于价值的决策/image-20240220233518921.png)

根据贝尔曼期望方程计算状态价值，由于采取确定性策略且状态转移是确定的，所以 $\pi(a^*\vert s)=1,P(s'\vert s,a)=1$ 
$$
\begin{aligned}
V(s)&=\sum\limits_{a\in A}\pi(a\vert s)\left(R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_\pi(s')\right)\\
&=R(s,a)+\gamma V_{\pi}(s')
\end{aligned}
$$
![image-20240220234156707](3-基于价值的决策/image-20240220234156707.png)

- k=0：

  ![image-20240220234539493](3-基于价值的决策/image-20240220234539493.png)

- k=1：

  ![image-20240220234817666](3-基于价值的决策/image-20240220234817666.png)

#### 价值迭代分析

##### 价值迭代理解

价值迭代每次迭代只能影响与之直接相关的状态，工作过程类似于价值的反向传播，每次迭代做进一步传播，如果子问题的价值变好了，当前价值也会变得更好。

**以最短路径为例** ：对于每个状态，都可以将其看最一个终点。每轮迭代，从每个状态开始根据贝尔曼最优方程重新计算价值。左上角为终点，每多走一步，价值越小。只有当 $V_7$ 收敛后，才能基于得到的最优价值来提取最佳策略

![image-20240124152431886](3-基于价值的决策/image-20240124152431886.png)

##### 中间状态价值/策略无意义

在贝尔曼方程中，$V^{(k+1)}$ 与 $V^{(k)}$ 都是状态价值；而在值迭代算法中，$V^{(k)}$ 可以是任意的值，不是状态价值

在未将每个状态的最优价值未传递给其他所有状态之前，中间的几个价值只是一种暂存的不完整的数据，不能代表每个状态的价值，所以基于中间过程的价值函数所生成的策略没有意义

### 3.1.2 策略迭代

#### 策略迭代算法

> 通过解决预测问题进而解决控制问题

1. 策略评估(policy evaluation,PE)：利用贝尔曼公式评估给定策略 $\pi^{(k)}$ 的状态价值
   $$
   \mathbf{V}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}
   $$

2. 策略改进(policy improvement,PI)：使用 $\mathop{\mathrm{argmax}}$ 进行策略改进
   $$
   \pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi\in\Pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi}\right)
   $$

##### 策略评估(预测问题)

利用贝尔曼方程计算状态价值
$$
\begin{aligned}
\mathbf{V}_{\pi^{(k)}}(s)&=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}(s),s\in \mathcal{S}
\end{aligned}
$$
其解析解为
$$
\mathbf{V}_{\pi^{(k)}}(s)=(\mathbf{I}-\gamma P_{\pi^{(k)}})^{-1}\mathbf{R}_{\pi^{(k)}}
$$
数值解为
$$
\begin{aligned}
\mathbf{V}_{\pi^{(k)}}^{(j+1)}(s)&=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}^{(j)}(s)\\
&=\sum\limits_{a\in A}\pi^{(k)}(a\vert s)\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_{\pi^{(k)}}^{(j)}(s')\right),s\in \mathcal{S},j=0,1,2,\cdots
\end{aligned}
$$

- 终止条件是 $\Vert \mathbf{V}_{\pi^{(k)}}^{(j+1)}(s)-\mathbf{V}_{\pi^{(k)}}^{(j)}(s)\Vert$ 足够小
- 通过自举法求解贝尔曼方程，相当于在策略迭代的大迭代过程中小迭代求解状态价值

##### 策略改进

> 由 **策略改进定理** ，可知 $argmax$ 可以得到单调递增的 $V$ 
>
> 若 $\pi'=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi}\right)$ ，则 $\mathbf{V}_{\pi'}\ge \mathbf{V}_{\pi}$

$$
\begin{aligned}
\pi^{(k+1)}&=\mathop{\mathrm{argmax}}\limits_{\pi\in\Pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi}\right)\\
&=\mathop{\mathrm{argmax}}\limits_{\pi\in\Pi}\sum\limits_{a}\pi(a\vert s)\underbrace{\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma \sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(k)}}(s')\right)}_{Q_{\pi^{(k)}}(s,a)},s\in \mathcal{S},s\in \mathcal{S}
\end{aligned}
$$

$Q_{\pi^{(k)}}(s,a)$ 为基于策略 $\pi^{(k)}$ 下在状态 $s$ 下的动作价值，最优动作 $a_{*}^{(k)}$ 
$$
a_{*}^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)
$$
因此，贪心策略为
$$
\pi^{(k+1)}(a\vert s)=\begin{cases}
1&a=a_{*}^{(k)}(s)\\
0&a\neq a_{*}^{(k)}(s)
\end{cases}
$$

##### 伪代码

变量序列为
$$
\pi^{(0)}\xrightarrow{PE}V_{\pi^{(0)}}\xrightarrow{PI}\pi^{(1)}\xrightarrow{PE}V_{\pi^{(1)}}\xrightarrow{PI}\cdots
$$
上述步骤经过多轮迭代，状态价值函数和策略都会收敛

![image-20240123173100717](3-基于价值的决策/image-20240123173100717.png)

$$
\begin{array}{ll}
\hline
&初始化:动态特性与状态转移P(r\vert s,a)与P(s'\vert s,a)，随机初始化\pi^{(0)}\\
&目标：寻找最优状态价值与最优策略\\
&当策略未收敛，第 k轮迭代:\\
&\quad 策略评估:\\
&\qquad 初始化：一个随机初始化的状态价值\mathbf{V}^{(0)}_{\pi^{(k)}}\\
&\qquad 当\mathbf{V}^{(j)}_{\pi^{(k)}} 未收敛，进行第j+1轮迭代：\\
&\quad\qquad 对每个状态 s\in\mathcal{S}:\\
&\qquad\qquad \mathbf{V}_{\pi^{(k)}}^{(j+1)}(s)=\sum\limits_{a\in A}\pi^{(k)}(a\vert s)\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_{\pi^{(k)}}^{(j)}(s')\right)\\
&\quad 策略改进:\\
&\qquad 对于每个状态s\in \mathcal{S}:\\
&\qquad \quad 对于每个动作 a\in \mathcal{A}(s):\\
&\qquad \qquad Q_{\pi^{(k)}}(s,a)=\sum\limits_{r'}P(r'\vert s,a)r'+\gamma \sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(k)}}(s')\\
&\qquad \quad a_*^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)\\
&\quad \qquad 若a_*^{(k)}(s)==a ,则\pi^{(k+1)}(a\vert s)=1，否则\pi^{(k+1)}(a\vert s)=0\\
\hline
\end{array}
$$

#### 策略迭代收敛性

策略迭代算法最终会得到收敛于最优策略的策略

在策略迭代过程中，会产生状态价值序列 $\{\mathbf{V}_{\pi^{(0)}},\mathbf{V}_{\pi^{(1)}},\cdots,\mathbf{V}_{\pi^{(k)}},\cdots\}$ 与策略序列 $\{\pi^{(0)},\pi^{(1)},\cdots,\pi^{(k)},\cdots\}$

由 **策略改进定理** ，状态价值是单调增的；而由于我们知道其存在解析解 $\mathbf{V}^*$ ，故有
$$
\mathbf{V}_{\pi^{(0)}}\le \mathbf{V}_{\pi^{(1)}}\le \cdots\le \mathbf{V}_{\pi^{(k)}}\le \cdots\le \mathbf{V}^*
$$
当 $k\rightarrow \infty$ 时，$\mathbf{V}^{(\infty)}=\mathbf{V}^*$ ，即状态价值会收敛于最优状态价值。因此，策略序列 $\{\pi^{(k)}\}_{k=0}^{\infty}$ 收敛于最优策略

##### 策略迭代的收敛速度快于价值迭代

策略迭代的状态价值序列为 $\{\mathbf{V}_{\pi^{(k)}}\}_{k=0}^{\infty}$ ，而价值迭代的状态价值序列为 $\{\mathbf{V}^{(k)}\}_{k=0}^{\infty}$ 
$$
\mathbf{V}^{(k+1)}=f(\mathbf{V}^{(k)})=\max\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)}\right)
$$
同理，我们也知道对于任意的初始值 $V^{(0)}$ ，有 $V^{(k)}\xrightarrow{k\rightarrow \infty} V^*$ 

若要证明 **策略迭代的迭代轮数 $k$ 小**，只需证明对于任一 $k$ ，有 $\mathbf{V}^{(k)}\le \mathbf{V}_{\pi^{(k)}}\le \mathbf{V}^*$ 

采用归纳法，假设对于 $k\ge 0$ ，有 $\mathbf{V}^{(k)}\le \mathbf{V}_{\pi^{(k)}}$ ，对于 $k+1$ 
$$
\begin{aligned}
\mathbf{V}_{\pi^{(k+1)}}-\mathbf{V}^{(k+1)}&=\left(\mathbf{R}_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}\mathbf{V}_{\pi^{(k+1)}}\right)-\max\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)}\right)\\
&\ge \left(\mathbf{R}_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}\mathbf{V}_{\pi^{(k)}}\right)-\max\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)}\right)\\
&\qquad(由于\mathbf{V}_{\pi^{(k+1)}}\ge \mathbf{V}_{\pi^{(k)}})\\
&=\left(\mathbf{R}_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}\mathbf{V}_{\pi^{(k)}}\right)-\left(\mathbf{R}_{\pi'_{(k)}}+\gamma P_{\pi'_{(k)}}\mathbf{V}_{\pi'_{(k)}}\right)\\
&\qquad 令 \pi'_{(k)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)})\\
&\ge \left(\mathbf{R}_{\pi'_{(k)}}+\gamma P_{\pi'_{(k)}}\mathbf{V}_{\pi^{(k)}}\right)-\left(\mathbf{R}_{\pi'_{(k)}}+\gamma P_{\pi'_{(k)}}\mathbf{V}_{\pi'_{(k)}}\right)\\
&\qquad 由于 \pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(k)}}\right)，大于任意策略值\\
&=\gamma P_{\pi'_{(k)}}\left(\mathbf{V}_{\pi^{(k)}}-\mathbf{V}^{(k)}\right)
\end{aligned}
$$
因此，$\mathbf{V}_{\pi^{(k+1)}}\ge\mathbf{V}^{(k+1)}$ 成立

故可知 $\mathbf{V}^{(k)}\le \mathbf{V}_{\pi^{(k)}}\le \mathbf{V}^*$ ，即策略迭代的收敛速度快

#### 策略迭代例子

![image-20240221202657024](3-基于价值的决策/image-20240221202657024.png)

状态空间有两个状态，$s_1,s_2\in \mathcal{S}$ ，动作空间 $\mathcal{A}=\{a_l,a_0,a_r\}$ 分别代表向左、不动、向右

奖励设置：$r_{boundary}=-1,r_{target}=1$ ，折扣因子 $\gamma=0.9$

初始策略 $\pi^{(0)}(s_1)=a_l,\pi^{(0)}(s_2)=a_l$ 

**$k=0$**

- 策略评估：根据贝尔曼方程计算状态价值 $\mathbf{V}_{\pi^{(0)}}$ 
  $$
  \begin{cases}
  V_{\pi^{(0)}}(s_1)=R_{\pi^{(0)}}(s_1)+\gamma V_{\pi^{(0)}}(s_1)\\
  V_{\pi^{(0)}}(s_2)=R_{\pi^{(0)}}(s_2)+\gamma V_{\pi^{(0)}}(s_1)
  \end{cases}=\begin{cases}
  V_{\pi^{(0)}}(s_1)=-1+\gamma V_{\pi^{(0)}}(s_1)\\
  V_{\pi^{(0)}}(s_2)=0+\gamma V_{\pi^{(0)}}(s_1)
  \end{cases}\xRightarrow{解析解}\begin{cases}
  V_{\pi^{(0)}}(s_1)=-10\\
  V_{\pi^{(0)}}(s_2)=-9
  \end{cases}
  $$
  若使用自举法求数值解，随机初始 $V_{\pi^{(0)}}^{(0)}(s_1)=V_{\pi^{(0)}}^{(0)}(s_2)=0$

  ![image-20240221204003812](3-基于价值的决策/image-20240221204003812.png)

  随着迭代次数 $j$ 的增多，$V_{\pi^{(0)}}^{(j)}(s_1)\rightarrow V_{\pi^{(0)}}(s_1)=-10,V_{\pi^{(0)}}^{(j)}(s_2)\rightarrow V_{\pi^{(0)}}(s_2)=-9$

- 策略改进，

  | $Q_{\pi^{(k)}}(s,a)$ | $a_l$                          | $a_0$                         | $a_r$                          |
  | -------------------- | ------------------------------ | ----------------------------- | ------------------------------ |
  | $s_1$                | $-1+\gamma V_{\pi^{(k)}}(s_1)$ | $0+\gamma V_{\pi^{(k)}}(s_1)$ | $1+\gamma V_{\pi^{(k)}}(s_2)$  |
  | $s_2$                | $0+\gamma V_{\pi^{(k)}}(s_1)$  | $1+\gamma V_{\pi^{(k)}}(s_2)$ | $-1+\gamma V_{\pi^{(k)}}(s_2)$ |

  当 $k=0$ 时，计算 $Q_{\pi^{(0)}}(s,a)$ 表

  ![image-20240221204628110](3-基于价值的决策/image-20240221204628110.png)

  策略改进为
  $$
  \pi^{(1)}(a\vert s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(0)}}(s,a)=\begin{cases}
  \pi^{(1)}(a_r\vert s_1)=1,\pi^{(1)}(a_0\vert s_1)=\pi^{(1)}(a_l\vert s_1)=0\\
  \pi^{(1)}(a_0\vert s_2)=1,\pi^{(0)}(a_0\vert s_1)=\pi^{(r)}(a_0\vert s_1)=0
  \end{cases}
  $$
  ![image-20240221205930439](3-基于价值的决策/image-20240221205930439.png)

**$k=1,2,\cdots$**

策略评估发现，$V_{\pi^{(k+1)}}(s_1)-V_{\pi^{(k)}}(s_1)\le \varepsilon,V_{\pi^{(k+1)}}(s_2)-V_{\pi^{(k)}}(s_2)\le \varepsilon$ ，则策略收敛

#### 通过策略迭代例子理解强化

$r_{boundary}=-1,r_{forbidden}=-10,r_{targer}=1,\gamma=0.9$ 

![image-20240221210420304](3-基于价值的决策/image-20240221210420304.png)

- $\gamma=0.9$ 更重视未来奖励，可能会造成选择短期负奖励，但可以更快地到达目标状态

  但再加上 $r_{forbidden}=-10$ ，加大了负奖励的惩罚，所以最优路径会绕过负奖励区域

- 离目标区域近的状态，其最优策略收敛早于远离目标区域的状态。
  只有当距离较近的状态能够找到到达目标的轨迹时，距离较远的状态才能找到穿过距离较近的状态到达目标的轨迹；若较近状态的策略并不是最优策略，较远状态做出的最优决策也是没有意义的

- 靠近目标的状态具有更大的状态值。这种模式的原因是，一个智能体从更远的状态出发，必须走很多步才能获得积极的奖励。这样的奖励将严重打折扣，因此相对较小。

### 3.1.3 截断策略迭代

#### 网格世界中的策略迭代与价值迭代

网格世界初始化界面

![image-20240124154153825](3-基于价值的决策/image-20240124154153825.png)

**策略迭代**

![image-20240124154228790](3-基于价值的决策/image-20240124154228790.png)

初始策略：每个状态采取固定的随机策略，每个状态可采取的各个动作等概率

第一次策略评估：每个状态都有一个价值函数

第一次策略改进：有些状态的策略已经发生改变

![image-20240124154658184](3-基于价值的决策/image-20240124154658184.png)

 第二次策略评估：在多次策略评估后，所有状态的价值函数收敛

第二次策略更新：每个状态会选择最佳策略来执行，不再是随机策略

![image-20240124155225933](3-基于价值的决策/image-20240124155225933.png)

直至每个格子的值不再变化，说明整个MDP已经收敛

**价值迭代**

![image-20240124155802457](3-基于价值的决策/image-20240124155802457.png)

当价值迭代使状态价值收敛后，可提取最佳策略

价值迭代提取出的最佳策略与策略迭代得出的最佳策略一致

#### 价值迭代与策略迭代对比

##### 变量序列

- 策略迭代

  策略评估，PE：$\mathbf{V}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}$ 

  策略改进，PI：$\pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(k)}})$

- 价值迭代

  策略更新，PU：$\pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(k)}})$ 

  价值更新，VU：$\mathbf{V}^{(k+1)}=\mathbf{R}_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}\mathbf{V}^{(k)}$

$$
\begin{aligned}
&策略迭代:\pi^{(0)}\xrightarrow{PE}&\mathbf{V}_{\pi^{(0)}}\xrightarrow{PI}\pi^{(1)}\xrightarrow{PE}\mathbf{V}_{\pi^{(1)}}\xrightarrow{PI}\pi^{(2)}\xrightarrow{PE}\mathbf{V}_{\pi^{(2)}}\xrightarrow{PI}\cdots\\
&价值迭代:&\mathbf{V}^{(0)}\xrightarrow{PU}\mu^{(1)}\xrightarrow{VU}\mathbf{V}^{(1)}\xrightarrow{PU}\mu^{(2)}\xrightarrow{VU}\mathbf{V}^{(2)}\xrightarrow{PU}\cdots
\end{aligned}
$$

##### 算法对比

|        | 策略迭代                                                     | 价值迭代                                                     | 备注                                                         |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Policy | $\pi^{(0)}$                                                  | 无 $\mu^{(0)}$                                               | 初始：策略迭代从策略 $\pi^{(0)}$ 出发，价值迭代从价值 $V^{(0)}$ 出发 |
| Value  | $\mathbf{V}_{\pi^{(0)}}=\mathbf{R}_{\pi^{(0)}}+\gamma P_{\pi^{(0)}}\mathbf{V}_{\pi^{(0)}}$ | 令 $V^{(0)}=V_{\pi^{(0)}}$                                   | 为便于比较(控制变量)，将价值迭代初始值设为贝尔曼方程的解     |
| Policy | $\pi^{(1)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(0)}})$ | $\mu^{(1)}=\mathop{\mathrm{argmax}}\limits_{\mu}(\mathbf{R}_{\mu}+\gamma P_{\mu}\mathbf{V}^{(0)})$ |                                                              |
| Value  | $\mathbf{V}_{\pi^{(1)}}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}$ | $\mathbf{V}^{(1)}=\mathbf{R}_{\mu^{(1)}}+\gamma P_{\mu^{(1)}}\mathbf{V}^{(0)}$ | 由策略改进定理，$\mathbf{V}_{\pi^{(1)}}\ge \mathbf{V}_{\pi^{(1)}}=V^{(0)}\Rightarrow \mathbf{V}_{\pi^{(1)}}\ge \mathbf{V}^{(1)}$ |
| Policy | $\pi^{(2)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(1)}})$ | $\mu^{(2)}=\mathop{\mathrm{argmax}}\limits_{\mu}(\mathbf{R}_{\mu}+\gamma P_{\mu}\mathbf{V}^{(1)})$ |                                                              |
| ...    | ...                                                          | ...                                                          | ...                                                          |

在价值迭代方法中，在价值更新步骤，只计算一轮就将 $V^{(1)}$ 代入下一轮策略更新

在策略迭代方法中，在价值评估步骤，利用贝尔曼方程求解当前策略下逼近解析解的数值解 $\mathbf{V}_{\pi^{(1)}}^{(j)}\xlongequal{j\rightarrow \infty}\mathbf{V}_{\pi^{(1)}}\rightarrow \mathbf{V}^*_{\pi^{(1)}}$ ；由策略改进定理，有 $\mathbf{V}_{\pi^{(1)}}\ge V^{(1)}$ 
$$
\begin{array}{rlll}
&&&\mathbf{V}_{\pi^{(1)}}^{(0)}=\mathbf{V}^{(0)}\\
价值迭代法&\leftarrow \mathbf{V}^{(1)}&\leftarrow&\mathbf{V}_{\pi^{(1)}}^{(1)}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}^{(0)}\\
&&&\mathbf{V}_{\pi^{(1)}}^{(2)}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}^{(1)}\\
&&&\vdots\\
截断策略迭代法&\leftarrow\overline{\mathbf{V}^{(1)}}&\leftarrow&\mathbf{V}_{\pi^{(1)}}^{(j)}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}^{(j-1)}\\
&&&\vdots\\
策略迭代法&\leftarrow \mathbf{V}_{\pi^{(1)}}&\leftarrow&\mathbf{V}_{\pi^{(1)}}^{(\infty)}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}^{(\infty)}
\end{array}
$$

- 因为策略评估需要经过无穷多步骤，所以策略迭代法只在理论上存在
- 实际编程实现的都是截断策略迭代算法，当相邻两次的价值函数差距小于一个阈值，则认为收敛

##### 其他方面对比

|            | 策略迭代                                                     | 价值迭代                                   |
| ---------- | ------------------------------------------------------------ | ------------------------------------------ |
| 贝尔曼方程 | 贝尔曼期望方程                                               | 贝尔曼最优方程                             |
| 策略提取   | 不管在那个状态，都可以利用状态对应的最佳策略到达可以获得最多奖励的状态 | 基于中间过程的价值函数所生成的策略没有意义 |
|            | 实时策略，在线学习                                           | 非实时策略，离线学习                       |

#### 截断策略迭代伪代码

$$
\begin{array}{ll}
\hline
&初始化:动态特性与状态转移P(r\vert s,a)与P(s'\vert s,a)，随机初始化\pi^{(0)}\\
&目标：寻找最优状态价值与最优策略\\
&当策略未收敛，第 k轮迭代:\\
&\quad 策略评估:\\
&\qquad 初始化：一个随机初始化的状态价值\mathbf{V}^{(0)}_{\pi^{(k)}}=V_{\pi^{(k-1)}}，设定迭代次数超参数j_{truncate}\\
&\qquad 当j<j_{truncate}，进行第j+1轮迭代：\\
&\quad\qquad 对每个状态 s\in\mathcal{S}:\\
&\qquad\qquad \mathbf{V}_{\pi^{(k)}}^{(j+1)}(s)=\sum\limits_{a\in A}\pi^{(k)}(a\vert s)\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_{\pi^{(k)}}^{(j)}(s')\right)\\
&\qquad 令V_{\pi^{(k)}}=V_{\pi^{(k)}}^{(j_{truncate})}
&\quad 策略改进:\\
&\qquad 对于每个状态s\in \mathcal{S}:\\
&\qquad \quad 对于每个动作 a\in \mathcal{A}(s):\\
&\qquad \qquad Q_{\pi^{(k)}}(s,a)=\sum\limits_{r'}P(r'\vert s,a)r'+\gamma \sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(k)}}(s')\\
&\qquad \quad a_*^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)\\
&\quad \qquad 若a_*^{(k)}(s)==a ,则\pi^{(k+1)}(a\vert s)=1，否则\pi^{(k+1)}(a\vert s)=0\\
\hline
\end{array}
$$

#### 截断策略迭代收敛性证明

策略评估中，迭代公式为
$$
\mathbf{V}^{(j+1)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}^{(j)}_{\pi^{(k)}},j=0,1,2,\cdots
$$
对于任一的初始值 $\mathbf{V}^{(0)}_{\pi^{(k)}}=V_{\pi^{(k-1)}}$ ，有
$$
\mathbf{V}^{(j+1)}_{\pi^{(k)}}\ge \mathbf{V}^{(j)}_{\pi^{(k)}},j=0,1,2,\cdots
$$
证明：

由于 $\mathbf{V}^{(j+1)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}^{(j)}_{\pi^{(k)}}$ 且 $\mathbf{V}^{(j)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}^{(j-1)}_{\pi^{(k)}}$ ，则有
$$
\mathbf{V}^{(j+1)}_{\pi^{(k)}}-\mathbf{V}^{(j)}_{\pi^{(k)}}=\gamma P_{\pi^{(k)}}\left(\mathbf{V}^{(j)}_{\pi^{(k)}}-\mathbf{V}^{(j-1)}_{\pi^{(k)}}\right)=\cdots=\gamma^j P_{\pi^{(k)}}^j\left(\mathbf{V}^{(1)}_{\pi^{(k)}}-\mathbf{V}^{(0)}_{\pi^{(k)}}\right)
$$
另外，由于 $\mathbf{V}^{(0)}_{\pi^{(k)}}=V_{\pi^{(k-1)}}$ 
$$
\mathbf{V}^{(1)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}^{(0)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}V_{\pi^{(k-1)}}\ge \mathbf{R}_{\pi^{(k-1)}}+\gamma P_{\pi^{(k-1)}}V_{\pi^{(k-1)}}\xlongequal{贝尔曼方程}\mathbf{V}_{\pi^{(k-1)}}=\mathbf{V}^{(0)}_{\pi^{(k)}}
$$
故可得，$\mathbf{V}^{(j+1)}_{\pi^{(k)}}\ge \mathbf{V}^{(j)}_{\pi^{(k)}},j=0,1,2,\cdots$

![image-20240221225836947](3-基于价值的决策/image-20240221225836947.png)

#### 截断策略迭代例子

![image-20240221230035376](3-基于价值的决策/image-20240221230035376.png)

策略评估求状态价值的迭代次数对收敛速度的影响

![image-20240221230121741](3-基于价值的决策/image-20240221230121741.png)

可见，在策略评估中，求解状态价值时，越多的迭代次数 $j_{truncate}$ ，则最终策略迭代会更快地收敛

但随着迭代次数的增加，带来的收敛加速度越来越小

在实践中，使用较小的迭代次数 $j_{truncate}$

## 3.2 蒙特卡洛方法——免模型

> 基于蒙特卡洛方法的免模型学习是一种 **广义策略迭代** (generalized policy iteration, GPI)
>
> - 不断地在 *策略评估, PE* 与 *策略改进, PI* 之间切换
> - 策略评估不需要精确的 $Q$ 值与 $V$ 值

- MC基本思想

- MC Basic：实现从有模型学习到无模型学习的转化
- MC探索性开始
  - 提高数据的使用效率
- 去除探索性开始条件
  - MC $\varepsilon-贪心$ 

### 3.2.1 蒙特卡洛基本思想

掷硬币，用随机变量 $X$ 表示结果，$+1,-1$ 表示两种结果，求期望 $E[X]$

- 基于模型的方法

  当我们已知硬币是均匀的，或任何能保证得出 $P(X=+1)=P(X=-1)=0.5$ 的条件，在这种先验知识的条件下，我们可以对实验环境进行建模，用 $P(X=+1)=P(X=-1)=0.5$  表示环境模型，进而可基于模型的方法估计期望

  $E[X]=\sum\limits_{x}xP(x)=0$ 

- 基于MC的方法：**采样取平均** 

  在不知道任何先验知识的前提下，就无法对实验环境进行建模

  无模型时用数据(经验,experience)：通过多次实验，可以得到样本集 $\{x_1,x_2,\cdots,x_N\}$ 
  $$
  E[X]\approx \overline{X}=\frac{1}{N}\sum\limits_{j=1}^Nx_j
  $$
  ![image-20240223105012439](3-基于价值的决策/image-20240223105012439.png)

  可见，随着实验次数的增多，$\overline{X}\xrightarrow{N\rightarrow \infty}E[X]$ 

#### MC方法理论依据——大数定律

对于一个随机变量 $X$ ，假设 $\{x_j\}_{j=1}^{N}$ 是独立同分布样本集，令 $\overline{X}=\frac{1}{N}\sum\limits_{j=1}^Nx_j$ ，则有，
$$
E[\overline{X}]=E[X]\\
Var[\overline{X}]=\frac{1}{N}Var[X]
$$
即 $\overline{X}$ 是 $E[X]$ 的无偏估计，且方差随着实验次数的增多减小

### 3.2.2 MC Basic

####   策略迭代分析

PE：$\mathbf{V}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}$

PI：$\pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(k)}}\right)$

对于策略改进步骤，针对每个状态 $s\in \mathcal{S}$
$$
\begin{aligned}
\pi^{(k+1)}(a\vert s)&=\mathop{\mathrm{argmax}}\limits_{\pi}\sum\limits_{a}\pi(a\vert s)\left[\sum\limits_{r'}P(r'\vert s,a)r'+\sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(k)}}(s')\right]\\
&=\mathop{\mathrm{argmax}}\limits_{\pi}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(k)}}(s,a)
\end{aligned}
$$
若环境动态特性 $P(s',r\vert s,a)$ 已知，可通过对动态规划方法计算 $Q_{\pi^{(k)}}(s,a)$ 

若环境动态特性 $P(s',r\vert s,a)$ 已知，则采用蒙特卡洛方法计算
$$
Q_{\pi^{(k)}}(s,a)=E_{\pi^{(k)}}[G_{t}\vert s_t=s,a_t=a]\\
\downarrow\\
均值估计问题\leftarrow MC方法
$$

#### MC Basic算法

1. 从 $(s,a)$ 开始，基于策略 $\pi^{(k)}$ 生成轨迹 $\tau$ 

2. 计算 $\tau$ 的回报 $g(s,a)$ 
   $$
   g^{(j)}(s,a)=r(s,a)+\gamma r(s_{t+1},a_{t+1})+\gamma^2r(s_{t+2},a_{t+2})+\cdots
   $$

   - 若采用确定性策略，且环境也是确定的。若估计 $Q(s_1,a_1)$ ，则基于当前策略 $\pi^{(k)}$ 从 $(s_1,a_1)$ 出发，不管采样多少次，得到的轨迹都是同一条，故只需要采样一次。因此估计整个Q表只需要45条轨迹，即 $N=1$ 
   - 若采用随机性策略或确定性策略但环境是随机的。若估计 $Q(s_1,a_1)$ ，则基于当前策略 $\pi^{(k)}$ 从 $(s_1,a_1)$ 出发，需要采集从$(s_1,a_1)$ 出发的 $N$ 条轨迹才能估计其状态价值 $Q(s_1,a_1)$ 。因此估计整个Q表共需计算 $45\times N$ 条轨迹的回报，即 $N$ 为足够大的数值

3. 执行 $N$ 次 1,2，可得到一个回报集 $\{g^{(j)(s,a)}\}_{j=1}^N$ ，计算动作价值
   $$
   Q_{\pi^{(k)}}(s,a)=E_{\pi^{(k)}}[G_{t}\vert s_t=s,a_t=a]\approx\overline{g}(s,a)=\frac{1}{N}\sum\limits_{j=1}^Ng^{(j)}(s,a)
   $$

##### 伪代码

$$
\begin{array}{ll}
\hline
&初始化：初始化策略 \pi^{(0)}\\
&目标：寻找最优策略\\
&当价值函数未收敛，进行第 k 轮迭代:\\
&\quad 对于每个状态 s\in \mathcal{S}:\\
&\qquad 对于每个动作a\in \mathcal{A}(s):\\
&\qquad \quad MC-based 策略评估:\\
&\qquad \qquad 以(s,a) 为起点，基于策略\pi^{(k)} 生成足够多的回合，计算这些回合的回报g(s,a)\\
&\qquad \qquad Q_{\pi^{(k)}}(s,a)=\overline{g}(s,a)\\
&\qquad MC-based 策略改进：\\
&\quad \qquad a^{(k)}_*=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)\\
&\quad \qquad 若a==a^{(k)}_*,则\pi^{(k+1)}(a\vert s)=1，否则\pi^{(k+1)}(a\vert s)=0\\
\hline
\end{array}
$$

#### MC Basic例子

![image-20240223155537861](3-基于价值的决策/image-20240223155537861.png)

- 初始策略 $\pi^{(0)}$ 
- 奖励设置 $r_{boundary}=-1,r_{forbidden}=-1,r_{target}=1,\gamma=0.9$ 

对于当前策略 $\pi^{(k)}$

1. 策略评估：计算 $Q_{\pi^{(k)}}(s,a)$ 

   对于示例，有9个状态，每个状态由5个动作，即 $Q$ 表需要计算 $9\times 5=45$ 个 $Q$ 值，由于此例采用确定性策略，所以只需要45条轨迹

2. 策略改进：贪心算法选择最优决策
   $$
   a_*^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)
   $$

以 $Q_{\pi^{(0)}}(s_1,a)$ 的计算为例

**1. 策略评估** 

从 $(s_1,a_1)$ 开始，轨迹为 $s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为
$$
Q_{\pi^{(0)}}(s_1,a_1)=-1+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{1}{1-\gamma}
$$
从 $(s_1,a_2)$ 开始，轨迹为 $s_1\xrightarrow{a_2}s_2\xrightarrow{a_3}s_5\xrightarrow{a_3}s_8\xrightarrow{a_2}s_9\xrightarrow{a_5}s_9\cdots$ ，动作价值为
$$
Q_{\pi^{(0)}}(s_1,a_2)=0+\gamma(0)+\gamma^2(0)+\gamma^3(1)+\cdots=\frac{\gamma^3}{1-\gamma}
$$
从 $(s_1,a_3)$ 开始，轨迹为 $s_1\xrightarrow{a_3}s_4\xrightarrow{a_2}s_5\xrightarrow{a_3}s_8\xrightarrow{a_2}s_9\xrightarrow{a_5}s_9\cdots$ ，动作价值为
$$
Q_{\pi^{(0)}}(s_1,a_3)=0+\gamma(0)+\gamma^2(0)+\gamma^3(1)+\cdots=\frac{\gamma^3}{1-\gamma}
$$
从 $(s_1,a_4)$ 开始，轨迹为 $s_1\xrightarrow{a_4}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为
$$
Q_{\pi^{(0)}}(s_1,a_4)=-1+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{1}{1-\gamma}
$$
从 $(s_1,a_5)$ 开始，轨迹为 $s_1\xrightarrow{a_5}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为
$$
Q_{\pi^{(0)}}(s_1,a_5)=0+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{\gamma}{1-\gamma}
$$
**2. 策略改进**

通过观察动作价值，可知在 $s_1$ 时，最佳动作是 $a_2,a_3$
$$
Q_{\pi^{(0)}}(s_1,a_2)=Q_{\pi^{(0)}}(s_1,a_3)
$$
因此，策略改进为
$$
\pi^{(1)}(a_2\vert s_1)或\pi^{(1)}(a_3\vert s_1)=1
$$

#### MC Basic特点

MCBasic算法是MC-based RL的核心，但由于其数据利用效率低，所以并不实用

因为策略迭代算法是收敛的，所以有足够回合数据的MC Basic也是收敛的

回合长度，**直观上可以理解为探索半径**。理论上越长越好，可以使计算出的回报更为精确，但实际上不可能无穷长，只要能让所有状态到达目标状态即可

##### 关于回合长度的例子

- 奖励设置 $r_{boundary}=-1,r_{forbidden}=-10,r_{target}=1,\gamma=0.9$ 

通过当前策略可到达目标状态的当前状态，其状态价值是正数；0表示未到达目标，若为负数，则表示有进入禁入区域或越界的中间状态

![image-20240223165057968](3-基于价值的决策/image-20240223165057968.png)

当回合长度为1时，只有目标状态一步之内的状态的最佳策略是正确的

当回合长度短时，只有离目标状态近的状态才有非0的状态价值，进而找到最优的策略；随着回合长度的增加，离目标状态远的状态才能到达目标状态，从而找到最优策略

![image-20240223165703357](3-基于价值的决策/image-20240223165703357.png)

![image-20240223165756497](3-基于价值的决策/image-20240223165756497.png)

### 3.2.3 探索性开始

#### 探索性开始的必要性

**探索**：确保每个 $(s,a)$ 都能被访问到

- 需要保证从一个访问出发有多个回合，只有这样才能用后边的奖励来估计回报，进一步估计动作价值
- 同时，为了不遗漏最优决策，需要保证每个$(s,a)$ 都能够被访问到

**开始** ：要生成以 $(s,a)$ 开始的回合

- 若要计算以 $(s,a)$ 开始的回合的回报，有两种方法

  - start：计算以(s,a) 开始的回合


  - visit 从其他 (s',a') 开始，中间经过 (s,a) ，将回合前截断，计算(s,a) 之后的回报

    但由于轨迹的生成依赖于策略与环境，无法保证以(s,a) 开始仍能获取后续轨迹，即不保证可复现

#### 提高策略评估的效率

> 访问：在每个回合中，每出现一次 $(s_t,a_t)$ 都为对其的一次访问

- 数据的高效利用
- 策略更新时机

##### 数据的高效利用

一个以 $(s_1,a_2)$ 开始的回合，仅用于计算 $Q(s_1,a_2)$ ，并为充分利用这一经验

**一个回合可以作为多个Q值的经验** 

![image-20240222174121229](3-基于价值的决策/image-20240222174121229.png)

采用 *first-visit* 方法，对于出现在同一个回合中的多个同一访问 $(s_t,a_t)$ ，只将其第一次出现之后的轨迹作为 $(s_t,a_t)$ 的一条轨迹

采用 *every-visit* 方法，对于出现在同一个回合中的多个同一访问 $(s_t,a_t)$ ， 将每个 $(s_t,a_t)$ 之后的轨迹都作为 $(s_t,a_t)$ 的一条轨迹

##### 策略更新时机——离线变在线

- 在策略评估中，若评估 $(s_1,a_1)$ 的动作价值 $Q(s_1,a_1)$ ，则需要收集 $N$ 条以 $(s_1,a_1)$ 开始的轨迹，然后计算这些轨迹的回报均值去近似动作价值

  被 MC Basic 算法采用，缺点是需要等所有的回合收集结束后再开始计算均值

- 使用一个回合的回报在线更新动作价值

  优点：可以在线逐回合改进策略

#### 伪代码：MC探索性开始

$$
\begin{array}{ll}
\hline
&初始化：初始化策略\pi^{(0)},Q(s,a),R(s,a)=0,Num(s,a)=0,\forall s\in \mathcal{S},a\in \mathcal{A}(s)\\
&目标：寻找最优策略\\
&\\
&对于每个回合:\\
&\quad生成回合:随机选择回合起点(s_0,a_0)，并确保所有的访问都有可能被选择到。\\
&\quad \qquad\qquad 基于当前策略 \pi^{(k)}生成回合长度为T的回合:s_0,a_0,r_0,\cdots,s_{T-1},a_{T-1},r_{T-1}\\
&\quad 初始化:g\leftarrow 0\\
&\quad 对于一个回合的每一步，t=T-1,T-2,\cdots,0:\\
&\qquad g\leftarrow \gamma g+r_{t+1}\\
&\qquad 使用first-visit方法:\\
&\quad \qquad若(s_t,a_t)未出现在前子轨迹(s_0,a_0,s_1,a_1,\cdots,s_{t_1},a_{t-1}),则:\\
&\qquad \qquad R(s_t,a_t)\leftarrow R(s_t,a_t)+g //采用第一次访问策略，只有第一次出现才算(s_t,a_t)的经验\\
&\qquad \qquad Num(s_t,a_t)\leftarrow Num(s_t,a_t)+1\\
&\qquad \qquad 策略评估：\\
&\quad\qquad \qquad Q(s_t,a_t)=average(R(s_t,a_t))\\
&\qquad \qquad 策略改进：\\
&\quad \qquad \qquad \pi(a^*\vert s_t)=1,其中a^*=\mathop{\mathrm{argmax}}\limits_{a}Q(s_t,a) 
\\
\hline
\end{array}
$$

### 3.2.4 MC无探索性开始

实际中，探索性开始很难实现。以每个 $(s,a)$ 为开始开始的回合收集成本很高

> 软策略,soft policy：每个动作都有概率被采用的策略，称为soft policy
> $$
> \begin{cases}
> 确定性策略\leftarrow 贪心算法\\
> 随机性策略\leftarrow 软策略
> \end{cases}
> $$

soft policies：去除探索性开始的条件

- 使用软策略，可以确保一些回合足够长，使得所有的 $(s,a)$ 都能出现足够的次数，只需要从一个或几个 $(s,a)$ 出发，就能覆盖到其他的访问，从而不需要大量的回合数据

#### $\varepsilon-贪心$ 

在强化学习中，使用的软策略为 $\varepsilon-贪心$ 算法
$$
\begin{aligned}
&\pi(a\vert s)=\begin{cases}
1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,对于最优动作\\
\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,对于其他 \vert \mathcal{A}(s)\vert-1个非最优动作
\end{cases}\\
&其中，\varepsilon\in[0,1]，\vert \mathcal{A}\vert(s)为状态s的动作数
\end{aligned}
$$

- 若 $\varepsilon=0.2$ ，$\vert \mathcal{A}\vert(s)=5$ ，则非最优动作的概率为 $\frac{0.2}{5}=0.04$ ，最优动作的概率为 $1-0.04\times 4=0.84$ 

当前状态 $s$ 下的最优决策有最大可能被选择，同时其他非最优决策也有一定的概率被选择到

最优决策被选择的概率比非最优决策被选择的概率大

- $1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)=1-\varepsilon+\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}\ge \frac{\varepsilon}{\vert \mathcal{A}\vert(s)}$

$\varepsilon-贪心$ 可以更好地平衡探索与利用

- $\varepsilon=0$ ，变为贪心策略，探索少但利用多
- $\varepsilon=1$ ，每个动作都被平等探索，探索多但利用少

#### 将 $\varepsilon-贪心$ 应用于强化学习

在MC Basic与MC探索性开始算法中，将策略改进步骤的贪心算法改为 $\varepsilon-贪心$ 算法

- 贪心算法 $\pi^{(k+1)}(a\vert s)=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(k)}}(s,a)$ 
  $$
  \pi^{(k+1)}(a\vert s)=\begin{cases}
  1,&a=a^{(k)}_*\\
  0,&a\neq a^{(k)}_*
  \end{cases}\\
  其中,a^{(k)}_*=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi}Q_{\pi^{(k)}}(s,a)
  $$

- $\varepsilon-贪心$ 算法 $\pi^{(k+1)}(a\vert s)=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi_{\varepsilon}}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(k)}}(s,a)$ 
  $$
  \pi^{(k+1)}(a\vert s)=\begin{cases}
  1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,a=a^{(k)}_*\\
  \frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,a\neq a^{(k)}_*
  \end{cases}
  $$

关于 $\Pi$ 与 $\Pi_{\varepsilon}$ 的理解

- $\Pi_{\varepsilon}$ 表示除最优决策外其他决策等可能的策略
- $\Pi$ 表示所有可能的策略

##### 伪代码

$$
\begin{array}{ll}
\hline
&初始化：初始化策略\pi^{(0)},\varepsilon\in [0,1],,Q(s,a),R(s,a)=0,Num(s,a)=0,\forall s\in \mathcal{S},a\in \mathcal{A}(s)\\
&目标：寻找最优策略\\
&\\
&对于每个回合:\\
&\quad 生成回合:随机选择回合起点(s_0,a_0),基于当前策略 \pi^{(k)}生成回合长度为T的回合:s_0,a_0,r_0,\cdots,s_{T-1},a_{T-1},r_{T-1}\\
&\quad 初始化:g\leftarrow 0\\
&\quad 对于一个回合的每一步，t=T-1,T-2,\cdots,0:\\
&\qquad g\leftarrow \gamma g+r_{t+1}\\
&\qquad 使用every-visit方法:\\
&\qquad \qquad R(s_t,a_t)\leftarrow R(s_t,a_t)+g\\
&\qquad \qquad Num(s_t,a_t)\leftarrow Num(s_t,a_t)+1\\
&\qquad \qquad 策略评估:\\
&\quad\qquad \qquad Q(s_t,a_t)=average(R(s_t,a_t))\\
&\qquad \qquad 策略改进:\\
&\quad\qquad \qquad 令a_*^{(k)}=\mathop{\mathrm{argmax}}\limits_{a}Q(s_t,a)，且\pi^{(k+1)}(a\vert s)=\begin{cases}
1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,a=a_*^{(k)}\\
\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,a\neq a_*^{(k)}
\end{cases}\\
\hline
\end{array}
$$

#### 特点

优点： $\varepsilon$ 越大，探索性越强，避开了探索性开始的条件

缺点：生成的最优策略的最优性越差

- 由于 $\varepsilon-贪心$ 生成的最优策略 $\pi^*_{\varepsilon}$ 仅是属于 $\Pi_{\varepsilon}$ 中的最优策略

- **$\varepsilon$不能太大**  ：当 $\varepsilon$ 小时，探索性小， $\varepsilon-贪心$ 就趋于贪心，用 $\varepsilon-贪心$ 找到的最优策略接近于贪心算法找到的最优策略

  **实际中，开始时 $\varepsilon$ 比较大，让其具有较强的探索能力；之后让 $\varepsilon$ 趋向于0确保得到的策略具有最优性**

#### 例子

##### $\varepsilon$ 越大，探索性越强

![image-20240224102256712](3-基于价值的决策/image-20240224102256712.png)

共125个 $(s,a)$，$\varepsilon$ 较大，探索性强，一个100 0000步的回合就探索了很多(s,a)

两个1000000步的回合就能找到基于 $\varepsilon-贪心$ 的最优策略 

![image-20240224094801791](3-基于价值的决策/image-20240224094801791.png)

##### 牺牲最优性

$\varepsilon-贪心$ 通过探索性得到了一些好处，但同时牺牲了一些状态的最优性

$r_{boundary}=-1,r_{forbidden}=-10,r_{target}=1,\gamma=0.9$

![image-20240222233322816](3-基于价值的决策/image-20240222233322816.png)

一致性：在每个状态下，采取最优动作的可能性最大

![image-20240224101526505](3-基于价值的决策/image-20240224101526505.png)

![image-20240224101639774](3-基于价值的决策/image-20240224101639774.png)

最优策略是基于最优状态价值定义的，所以最优状态价值在反映策略的好坏

虽然所有的策略都与最优策略保持一致，但由于还可能采取其他非最优动作，所以状态价值会比贪心算法的最优状态价值小，其最优性越来越差

在实际应用中，我们会将基于  $\varepsilon-贪心$ 算法的最优策略转为基于贪心算法的策略，希望这个策略与最优策略保持一致。但随着 $\varepsilon$ 的增大，二者已不具有一致性，所以 $\varepsilon$ 不能太大。一个技巧就是$\varepsilon$ 先大后小

## 3.3 随机近似与随机梯度下降

> **时序差分方法** 是随机近似的一种特殊情况

1. 均值近似的不同理解

   使用数据集 $\{x_k\}$ 估计均值 $E[X]$ 
   $$
   w_{k+1}=w_k-\frac{1}{k}(w_k-x_k)
   $$

2. Robbins-Monro 算法，RM算法

   使用观测集 $\{\tilde{g}(w_k,\eta_k)\}$ 估计未知方程 $g(w)=0$ 的解
   $$
   w_{k+1}=w_k-a_k\tilde{g}(w_k,\eta_k)
   $$

   - 算法描述
   - 示例
   - 收敛性分析
   - 应用于均值近似

3. 随机梯度下降——RM算法的一种特殊情况

   使用梯度集 $\{\bigtriangledown_wf(w_k,x_k)\}$ 求解最优化问题 $\min\limits_{w}J(w)=\min\limits_{w}E[f(w,x)]$ 的最优解，令 $g(w)=E[\bigtriangledown_wf(w,x)]=0$ ，梯度 $\bigtriangledown_wf(w_k,\eta_k)$ 相当于期望的带误差观测
   $$
   w_{k+1}=w_k-a_k\bigtriangledown_wf(w_k,\eta_k)
   $$

   - 算法描述
   - 示例
   - 收敛性
   - 确定性公式
   - BGD，MBGD，SGD

### 3.3.1 均值近似的不同理解

> 对于一个随机变量，目标是估计其期望 $E[X]$ ，假设有一组独立同分布的数据集 $\{x_i\}_{i=1}^N$ ，期望可以用其均值近似 $E[X]\approx \overline{X}=\frac{1}{N}\sum\limits_{i=1}^Nx_i$ 

强化学习中有很多期望，除价值函数外，还有很多量需要用数据去估计

#### 增量式均值计算方法

- 收集所有的轨迹，再计算其均值：必须等 $N$ 条数据全部都收集完后，才能取平均
- 增量式(迭代式)

对于非增量式计算
$$
w_{k+1}=\frac{1}{k}\sum\limits_{i=1}^kx_i,k=1,2,\cdots\\
w_{k}=\frac{1}{k-1}\sum\limits_{i=1}^{k-1}x_i,k=2,3\cdots
$$
因此有
$$
\begin{aligned}
w_{k+1}&=\frac{1}{k}\sum\limits_{i=1}^kx_i=\frac{1}{k}\left(\sum\limits_{i=1}^{k-1}x_i+x_k\right)\\
&=\frac{1}{k}((k-1)w_{k}+x_k)\\
&=w_k-\frac{1}{k}(w_k-x_k)
\end{aligned}
$$
利用上式可以增量式计算均值 $\overline{X}$ 
$$
\begin{array}{ll}
当有一个数据 &w_1=x_1\\
当有一个数据 &w_2=x_1\\
当有两个数据 &w_3=w_2-\frac{1}{2}(w_2-x_2)=\frac{1}{2}(x_1+x_2)\\
当有三个数据 &w_4=w_3-\frac{1}{3}(w_3-x_3)=\frac{1}{3}(x_1+x_2+x_3)\\
&\vdots\\
当有k个数据 &w_{k+1}=w_k-\frac{1}{k}(w_k-x_k)=\frac{1}{k}(x_1+x_2+\cdots+x_k)=\frac{1}{k}\sum\limits_{i=1}^kx_i
\end{array}
$$

##### 特点

- 增量式均值估计每采样一次都能更新一次
- 均值估计一开始由于数据量不充分，所以并不精确 $(w_{k+1\neq E[X]})$ 。但随着样本数的增多，估计会越来越准确 $(w_{k+1}\xrightarrow{k\rightarrow N}E[X])$ 

#### 广义增量式均值估计

$$
w_{k+1}=w_{k}-a_{k}(w_{k}-x_k)
$$

将 $\frac{1}{k}$ 替换为 $\alpha_k$ ，仍可收敛于 $E[X]$ 

- 属于特殊的 **随机近似算法**
- 属于 **随机梯度下降算法**

### 3.3.2 RM算法

随机近似算法 (stochastic approximation, SA) ：用于方程求解或最优化问题的一类随机的、迭代的算法

- 相较于其他方程求解算法(梯度上升/下降算法)，随机近似算法不需要知道方程的具体形式
  - 方程的表达式 $g$ 是已知的，可以通过很多数值算法求解
  - 方程的表达式 $g$ 是未知的（神经网络：已知输入输出，但不知道具体的网络结构）


Robbins-Monro算法：是SA中的一种代表性算法

- 随机梯度下降算法、增量式均值估计都属于RM算法

方程求解问题应用于最优化求解：求解 $J(w)$ 的最优化问题，利用梯度下降法求解 $g(w)=\bigtriangledown_w J(w)=0$ 也是一个方程求解问题

**问题定义** 

求解表达式未知的方程的根
$$
g(w)=0
$$
其中，$w$ 为变量，$g(\cdot)$ 为表示未知方程的函数，$w^*$ 为方程的解

**求解** 

RM算法是一种迭代式算法
$$
\begin{aligned}
w_{k+1}&=w_{k}-a_k\widetilde{g}(w_{k},\eta_{k})\\
&=w_k-a_k[g(w_{k})+\eta_{k}]
\end{aligned}
$$

- $w_{k}$ 表示对 $w$ 的第 $k$ 次迭代
- 函数 $g(\cdot)$ 是一个黑盒函数， $g(w_k)$ 的准确值是未知的，但我们可以观测到关于 $g(w_k)$ 的带有噪音的观测值 $\tilde{g}(w_k,\eta_k)$ 

通过RM算法求解方程的解依赖于数据

- 输入序列：$\{w_{k}\}$ 
- 带噪音的输出序列：$\{\widetilde{g}(w_{k},\eta_{k})\}$ 

#### 收敛性

##### 例子

估计 $g(w)=\tanh(w-1)=0$ 的解，初始 $w_1=3,a_k=\frac{1}{k},\eta_k\equiv0$ (为简化计算，假设无噪音干扰)

![Figure_1](3-基于价值的决策/Figure_1.png) 

有解析解 $w^*=1$ 

由于噪音是0，所以本例中 $\tilde{g}(w_k,\eta_k)=g(w_k)$ ，基于以下公式计算

$$
w_{k+1}=w_k-a_kg(w_k)
$$


![image-20240225111350459](3-基于价值的决策/image-20240225111350459.png)

$w_2=w_1=3$ ，$w_3=w_2-a_2g(w_2)=3-\frac{1}{2}\tanh(3)=3-\frac{0.9950547536867305}{2}=2.5024726231566348$

直观上看，$w_{k+1}$ 会随着 $k$ 的迭代不断接近 $w^*$ 

- 当 $w_k>w^*$ 时，有 $g(w_k)>0$ ， $w_{k+1}=w_k-a_kg(w_k)<w_k$ 
-  当 $w_k<w^*$ 时，有 $g(w_k)<0$ ， $w_{k+1}=w_k-a_kg(w_k)>w_k$ 

因此，$w_{k+1}$ 比 $w_k$ 更靠近 $w^*$ 

##### 证明

RM算法 $w_{k+1}=w_{k}-a_k\widetilde{g}(w_{k},\eta_{k})$​ 收敛的充分条件是：

> - $0< c_1\le \bigtriangledown_w g(w)\le c_2,\forall w$ 
>- $\sum\limits_{k=1}^{\infty}a_k=\infty$ 且 $\sum\limits_{k=1}^{\infty}a_k^2<\infty$ 
> - $E[\eta_k\vert \mathcal{H}_k]=0$ 且 $E[\eta_k^2\vert \mathcal{H}_k]<\infty$ ，$\mathcal{H}_k=\{w_k,w_{k-1,\cdots}\}$ 
> 
> 则 $w_k$ 依概率收敛于 $w^*$ 为方程的解 $g(w^*)=0$ 

**1.  $0< c_1\le \bigtriangledown_w g(w)\le c_2,\forall w$ 的条件分析** 

- 导数大于0的条件：确保单调增，一定会有0点
- 导数有界，避免无穷的情况

为什么单调增的 $g(w)$ 是可以接受的：随机近似常用于最优化方程的求解，即 $g(w)=\bigtriangledown_w J(w)=0$ ，对于凸优化问题，$J''(w)\ge 0\iff g'(w)>0$ ，所以是可接受的

**2.  $E[\eta_k\vert \mathcal{H}_k]=0$ 且 $E[\eta_k^2\vert \mathcal{H}_k]<\infty,\mathcal{H}_k=\{w_k,w_{k-1,\cdots}\}$ **

噪音 $\{\eta_k\}$ 是一个独立同分布的随机序列，其均值应该为0($E[\eta_k\vert \mathcal{H}_k]=0$)，且方差应是有界的($E[\eta_k^2\vert \mathcal{H}_k]<\infty,\mathcal{H}_k=\{w_k,w_{k-1,\cdots}\}$)，且 $\eta$ 并不需要服从高斯分布

 **3. $\sum\limits_{k=1}^{\infty}a_k=\infty$ 且 $\sum\limits_{k=1}^{\infty}a_k^2<\infty$ 的条件分析**

- $\sum\limits_{k=1}^{\infty}a_k^2<\infty$ 确保 $a_k\xrightarrow{k\rightarrow \infty}0$ 

  由于 $w_{k+1}-w_{k}=-a_k\tilde{g}(w_{k},\eta_{k})$ ，若 $a_k\rightarrow 0\Rightarrow a_k\tilde{g}(w_{k},\eta_{k})\rightarrow 0\Rightarrow w_{k+1}-w_{k}\rightarrow 0$ ，若想要 $w_k$ 收敛，则 $w_{k+1}-w_{k}\rightarrow 0$ 是一个充分条件条件

  若 $w_k\rightarrow w^*$，则有 $g(w_k)\rightarrow 0$ 并且 $\tilde{g}(w_{k},\eta_{k})$ 受 $\eta_k$ 扰动

- $\sum\limits_{k=1}^{\infty}a_k=\infty$ 确保 $a_k$ 趋于0的速度不会过快

  由上，$w_2=w_1-a_1\tilde{g}(w_1,\eta_1)$ ，$w_3=w_2-a_2\tilde{g}(w_2,\eta_2)$ ，$\cdots ,w_{k+1}=w_k-a_k\tilde{g}(w_k,\eta_k)$ ，等号左右分别求和
  $$
  w_{k+1}-w_1=\sum\limits_{k=1}^{k}-a_k\tilde{g}(w_k,\eta_k)
  $$
  若 $w_{\infty}=w^*$ ，假设 $\sum\limits_{k=1}^{\infty}a_k<\infty$ ，则 $\sum\limits_{k=1}^{\infty}a_k\tilde{g}(w_k,\eta_k)$ 是有界的，则 $w^*$ 与 $w_{1}$ 的差距是有界的，意味着不能随便选择初始的 $w_1$ 。若选择的 $w_1$ 与精确值间距过大，超过这个界，则等式不成立，即 $w_{\infty}$ 不是方程的解

  当 $\sum\limits_{k=1}^{\infty}a_k=\infty$ ，表示选择的初始 $w_1$ 不管离 $w^*$ 有多远，最后都能收敛到精确值

##### 满足系数的条件的 $a_k$

典型的是 $a_k=\frac{1}{k}$ ，级数 $\sum\limits_{k=1}^n\frac{1}{k}$ 在 $n\rightarrow \infty$ 时是发散的，且 $\sum\limits_{k=1}^{\infty}\frac{1}{k^2}=\frac{\pi^2}{6}<\infty$ 

在强化学习中，$a_k$ 会选择足够小的常量，不会使用 $\frac{1}{k}$ ，这样会使 $\sum\limits_{k=1}^{\infty}a_k^2<\infty$ 不成立，但算法依然会是有效的

- 后续的数据，随 $k$ 的增大，其作用会非常小。在实际问题中，我们希望后续的数据也有用，所以不会让 $a_k\xrightarrow{k\rightarrow \infty}0$ ，而让其 $a_k\xrightarrow{k\rightarrow \infty}\varepsilon$ ，$\varepsilon$ 是一个非常小的数

#### RM算法应用于均值估计

$$
w_{k+1}=w_k-a_k(w_k-x_k)
$$

我们知道若 $a_k=\frac{1}{k}$ ，则 $w_{k+1}=\frac{1}{k}\sum\limits_{i=1}^kx_i\xrightarrow{k\rightarrow \infty}E[X]$ 。但 $a_k\neq\frac{1}{k}$ 时，$w_k$ 是否仍能收敛与期望 $E[X]$ 目前并未分析过。基于RM算法，只需证明 **广义增量式均值估计** 属于RM算法即可。

在均值估计中，$g(w)=w_k-E[X]=0$ ，观测量 $\tilde{g}(w,\eta)=w-x$ 
$$
\begin{aligned}
\tilde{g}(w_k,\eta_k)&=w_k-x_k=w_k-E[X]+E[X]-x_k\\
&=(w_k-E[X])+(E[X]-x_k)=g(w_k)+\eta_k
\end{aligned}
$$
### 3.3.3 随机梯度下降

**问题定义** 
$$
\min\limits_{w} J(w)=\min\limits_{w}E[f(w,X)]
$$

$w$ 是最优化参数，$X$ 是随机变量，优化目标是以 $w$ 为参数最小化 $f(w,X)$ 的期望

- 期望可以看做带权求和的函数，待解决的问题可以看做多元函数最优化问题

**方法**

1. 梯度下降法（gtradient descent,GD）

   找目标函数的最小值，所以使用梯度下降法
   $$
   w_{k+1}=w_k-a_k\underbrace{\bigtriangledown_w E[f(w,X)]}_{\bigtriangledown_w J(w)}\xlongequal{期望本质上是求和}w_k-a_kE[\bigtriangledown_wf(w,X)]
   $$
   沿梯度方向，函数值减少最快，可以更快地收敛到最小值

   缺点：期望的计算

   - 有模型，用公式求
   - 无模型，用数据求

2. 批量梯度下降（batch gradient descent, BGD）

   MC方法： $E[\bigtriangledown_wf(w,X)]\approx\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_i,x_i)$ 
   $$
   w_{k+1}=w_k-a_k\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_i,x_i)
   $$
   缺点：每次迭代 $w_k$ 都需要采样很多次数 $N$ 次

3. 随机梯度下降（stochastic gradient descent, SGD）
   $$
   w_{k+1}=w_k-a_k\bigtriangledown_wf(w_k,x_k)
   $$

   - 将 **梯度下降法** 中，真实的梯度替换为对梯度的采样 $\bigtriangledown_w f(w_k,x_k)$ 
   - 将 **批量梯度下降法** 中，令 $n=1$

#### 算法示例

相当于最小化目标 $w$ 与变量 $X$ 距离的期望
$$
\min\limits_{w}J(w)=\min\limits_{w}E[f(w,X)]=\min\limits_{w}E\left[\frac{1}{2}\Vert w-X \Vert^2\right]
$$
**1. 最优解 $w^*$ 为 $E[X]$** 

求解最小化 $\min\limits_{w}J(w)$ ，即令 $\bigtriangledown_wJ(w)=0$ 
$$
\bigtriangledown_wJ(w)=0\Rightarrow \bigtriangledown_wE[f(w,X)]=0\Rightarrow E[\bigtriangledown_wf(w,X)]=0\Rightarrow E[w^*-X]=0\Rightarrow w^*=E[X]
$$
**2. 梯度下降法求解最小值**
$$
\begin{aligned}
w_{k+1}&=w_k-a_k\bigtriangledown_wJ(w_k)\\
&=w_k-a_kE[\bigtriangledown_w f(w_k,X)]\\
&=w_k-a_kE[w_k-X]\\
&=w_k-a_kw_k+a_kE[X]
\end{aligned}
$$
每轮 $w_k$ 都需要估计一次期望 $E[X]$ 

**3. 随机梯度下降法求解最小值** 
$$
w_{k+1}=w_k-a_k\bigtriangledown_wf(w_k,x_k)=w_k-a_k(w_k-x_k)
$$

#### 收敛性

> 在梯度下降法中，目标是求解最小值，所以沿着  $w_k$  梯度方向会更快地下降到  $w_k$ 方向上的最小值，我们知道 $w_{k}\xrightarrow{k\rightarrow \infty}w^*$；而随机梯度下降法，用 $(w_k,x_k)$ 处的梯度代替取 $w=w_k$ 时所有点 $(w_k,x_i),\forall i$ 处的梯度均值，即一个数据代替数据整体的梯度
> $$
> GD:w_{k+1}=w_k-a_kE[\bigtriangledown_wf(w_k,X)]\\
> \Downarrow\\
> SGD:w_{k+1}=w_k-a_k\bigtriangledown_wf(w_k,x_k)
> $$
> 因为 $\bigtriangledown_wf(w_k,x_k)\neq E[\bigtriangledown_wf(w_k,X)]$ ，需要证明基于SGD，$w_k\xrightarrow{\infty}w^*$ 是否成立  

证明思路： **随机梯度下降法** 是一个特殊的 **随机近似算法** ，且满足随机近似收敛的充分条件，则 **随机梯度下降法** 也会收敛

优化目标是最小化期望
$$
\min\limits_wJ(w)=\min\limits_{w} E[f(w,X)]
$$
最优化问题使用梯度下降法，等价于求解方程 $\bigtriangledown_w J(w)=E[\bigtriangledown_wf(w_k,X)]=0$ 的解 $w^*$ 

对于这么一个未知方程的求解问题，可以用RM算法求解，其观测量为 $\tilde{g}(w,\eta)$
$$
\begin{aligned}
\tilde{g}(w,\eta)&=\bigtriangledown_w f(w,x)\\
&=\underbrace{E[\bigtriangledown_wf(w,X)]}_{g(w)}+\underbrace{\bigtriangledown_wf(w,x)-E[\bigtriangledown_wf(w,X)]}_{\eta}
\end{aligned}
$$

- $\bigtriangledown_wf(w_k,x_k)$ 可以看做 $E[\bigtriangledown_wf(w_k,X)]$ 的一个噪音度量

因此，SGD是一种特殊的 RM 算法。若满足RM算法收敛的三个条件

- $0<c_1\le \bigtriangledown^2_w f(w,X)\le c_2$
- $\sum\limits_{k=1}^{\infty}a_k=\infty$ 且 $\sum\limits_{k=1}^{\infty}a_k^2<\infty$ 
- $\{x_k\}_{k=1}^{\infty}$ 是独立同分布的 

则SGD一定收敛于 $w^*$

##### SGD收敛的性质

SGD中，用随机梯度代替真实梯度，梯度的随机性并不会造成收敛的随机性或慢收敛，即初始值的选择 $w_0$ 是否会影响收敛速度

- 当 $w_k$ 与 $w^*$ 差距大时 ，SGD表现与GD相同，因为相对误差比较小
- 当 $w_k$ 与 $w^*$ 差距小时，SGD才会呈现较大的随机性

引入随机梯度与批量梯度的相对误差
$$
\delta_k=\frac{\vert \bigtriangledown_wf(w_k,x_k)- E[\bigtriangledown_w f(w_k,X)]\vert}{\vert E[\bigtriangledown_w f(w_k,X)]\vert}
$$
对于最优解 $w^*$ 满足梯度 $E[\bigtriangledown_w f(w^*,X)]=0$ ，进而有
$$
\delta_k=\frac{\big\vert \bigtriangledown_wf(w_k,x_k)- E[\bigtriangledown_w f(w_k,X)]\vert}{\vert E[\bigtriangledown_w f(w_k,X)]-E[\bigtriangledown_w f(w^*,X)]\vert}=\frac{\vert \bigtriangledown_wf(w_k,x_k)- E[\bigtriangledown_w f(w_k,X)]\vert}{\big\vert E[\left(\bigtriangledown_w f(w_k,X)-\bigtriangledown_wf(w^*,X)\right)]\big\vert}\xlongequal{中值定理}\frac{\vert \bigtriangledown_wf(w_k,x_k)- E[\bigtriangledown_w f(w_k,X)]\vert}{\big\vert E[\bigtriangledown_w^2 f(\widetilde{w_k},X)(w_k-w^*)]\big\vert},\widetilde{w_k}\in [w_k,w^*]
$$
假设 $f$ 是一个严格凸函数，$\bigtriangledown_w^2 f\ge c>0,\forall w,X$ ，则相对误差的分母为
$$
\begin{aligned}
\big\vert E[\bigtriangledown_w^2 f(\widetilde{w_k},X)(w_k-w^*)]\big\vert&=\big\vert E[\bigtriangledown_w^2 f(\widetilde{w_k},X)](w_k-w^*)\big\vert\\
&=\big\vert E[\bigtriangledown_w^2 f(\widetilde{w_k},X)]\big\vert\big\vert(w_k-w^*)\big\vert\ge c\vert(w_k-w^*)\vert
\end{aligned}
$$
代入相对误差
$$
\delta_k\le \frac{\vert \overbrace{\overbrace{\bigtriangledown_wf(w_k,x_k)}^{随机梯度}- \overbrace{E[\bigtriangledown_w f(w_k,X)]}^{真实梯度}}^{绝对误差}\vert}{c\underbrace{\vert(w_k-w^*)\vert}_{与最优解的间距}}
$$
相对误差 $\delta_k$ 反比于与最优解的间距：

- 分母大的时候，相对误差上界相对小，绝对误差也相对小
- 分母小的时候，相对误差上界相对大，绝对误差也相对大

当 $w_k$ 与 $w^*$ 距离远时，SGD的参数调整方向与GD的方向大致相同

#### SGD的确定性公式

常见的最优化问题是
$$
\min_w J(w)=\min_w\frac{1}{n}\sum\limits_{i=1}^nf(w,x_i)
$$
$x_i$ 并不是一个随机变量，只是来源于独立同分布的实数集 $\{x_i\}_{i=1}^n$ ，使用梯度下降法最优化这样的带参函数 $f(w,x_i)$ 
$$
w_{k+1}=w_k-a_k\bigtriangledown_w J(w)=w_k-a_k\sum\limits_{i=1}^n\frac{1}{n}\bigtriangledown_wf(w,x_i)
$$
假设数据集很大，即 $n$ 很大，则可使用增量式梯度下降——SGD确定性公式
$$
w_{k+1}=w_k-a_k\bigtriangledown_w J(w)=w_k-a_k\bigtriangledown_wf(w_k,x_k)
$$
这样的梯度下降公式虽然与 SGD相似，但它不含期望与随机变量。通过引入一个随机变量将确定性最优化问题变为随机最优化问题，假设 $X$ 是一个由 $\{x_i\}_{i=1}^n$ 定义的随机变量，且服从均匀分布 $P(X=x_i)=\frac{1}{n}$ 
$$
\min\limits_{w}J(w)=\min\limits_{w}\frac{1}{n}\sum\limits_{i=1}^nf(w,x_i)=\min_{w}E[f(w,X)]
$$
所以也可以用SGD求解最优化，即上述SGD确定性公式

- 关于 $x_k$ 的选取：从 $\{x_i\}_{i=1}^n$ 中有放回地随机取出 $x_k$ 

### 3.3.4 BGD、SGD、MBGD对比

对于同一个最优化问题 $\min\limits_{w}J(w)=\min_{w}E[f(w,X)]$ ，随机变量 $X$ 的样本集为 $\{x_i\}_{i=1}^n$  
$$
\begin{array}{ll}
w_{k+1}=w_k-a_k\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_k,x_i)&(BGD)\\
w_{k+1}=w_k-a_k\frac{1}{m}\sum\limits_{x_i\in \mathcal{I}_k}\bigtriangledown_wf(w_k,x_i)&(MBGD)\\
w_{k+1}=w_k-a_k\frac{1}{n}\bigtriangledown_wf(w_k,x_k)&(SGD)\\
\end{array}
$$
BGD，每轮迭代需要 $n$ 个数据，最优化结果最接近真实的最优解

MBGD，从 $n$ 个数据中采集 $m<n$ 个数据生成数据集 $\mathcal{I}_k$ ，作为第 $k$ 轮迭代的数据，基于这个子集的梯度计算最优解

SGD，每轮迭代随机选择一个数据，计算其梯度

相较于SGD，MBGD使用了更多数据将噪音等测量平均掉，所以随机性更小

相较于BGD，MBGD不需要使用全部数据，所以更加灵活和高效

- 当 $m=1$ ，MBGD变为SGD

- 当 $m=n$ ，MBGD变为BGD

  BGD用到 $n$ 个数据，虽然 MBGD也会用 $n$ 个数据，但是在BGD的 $n$ 个数据中有重复采样

#### 算法示例

对于给定的数据集 $\{x_i\}_{i=1}^{n}$ ，目标是最优化 $\overline{X}=\frac{1}{n}\sum\limits_{i=1}^nx_i$ ，数学公式表示为
$$
\min\limits_{w}J(w)=\min\limits_{w}\frac{1}{2n}\sum\limits_{i=1}^n\Vert w-x_i\Vert^2
$$
使用梯度下降法求解，其中 $\bigtriangledown_wJ(w)=\frac{1}{n}\sum\limits_{i=1}^n(w-x_i)$ 
$$
\begin{array}{ll}
w_{k+1}=w_k-a_k\frac{1}{n}\sum\limits_{i=1}^n(w_k-x_i)=w_k-a_k(w_k-\overline{x})&(BGD)\\
w_{k+1}=w_k-a_k\frac{1}{m}\sum\limits_{i=1}^{m}(w_k-x_i)=w_k-a_k(w_k-\overline{x}_k^{(m)})&(MBGD)\\
w_{k+1}=w_k-a_k(w_k-x_k)&(SGD)\\
\end{array}
$$

##### 收敛速度对比

$X\in \R^2$ 表示一个平面中的随机位置，其分布服从 $U(20,20)$ 的均匀分布，则真实均值 $E[X]=\mathbf{0}$，基于100个独立同分布的样本 $\{x_i\}_{i=1}^{100}$ ，令 $a_k=\frac{1}{k}$，求均值

![image-20240227163416632](3-基于价值的决策/image-20240227163416632.png)

![image-20240227163505422](3-基于价值的决策/image-20240227163505422.png)

观察橙色线（基于随机梯度下降的最优化），尽管初始值与真实期望差距很大，但SGD朝着最优解的方向收敛，当接近最优解时，呈现随机性，但仍逐渐收敛于真实值

若 $a_k=\frac{1}{k}$ 
$$
\begin{array}{ll}
w_{k+1}=w_k-\frac{1}{k}(w_k-\overline{x})\xlongequal{迭代推导}\frac{1}{k}\sum\limits_{i=1}^kx_i=\frac{1}{k}\sum\limits_{i=1}^k\overline{x}=\overline{x}&(BGD)\\
w_{k+1}=\frac{1}{k}\sum\limits_{i=1}^k\overline{x}_k^{(m)}=\overline{x}_k^{(m)}&(MBGD)\\
w_{k+1}=\frac{k-1}{k}w_k+\frac{1}{k}x_k=\frac{1}{k}\sum\limits_{i=1}^kx_i=\frac{1}{k}\overline{x}&(SGD)  
\end{array}
$$
BGD的每一步迭代都是最优解 $w^*=\overline{x}$

由于 $\overline{x}_k^{(m)}$ 已经是一个均值，所以MBGD收敛于均值的速度快于SGD，且 $m$ 越大，MBGD的收敛速度越快







