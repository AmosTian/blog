---
categories:
  - AI
  - 机器学习
tags:
  - AI
  - 机器学习
top: 8
mathjax: true
title: 1. 机器学习绪论
abbrlink: 1771285239
date: 2023-08-21 15:27:44
---

[TOC]

---

机器学习是计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的学科

根据输入输出类型的不同，机器学习分为：分类问题，回归问题，标注问题三类

过拟合是机器学习中不可避免的，可通过选择合适的模型降低影响

监督学习是机器学习的主流任务，包括生成方法和判别方法两类

<!--more-->

## 1.1 机器学习概念

人类学习机制：从大量现象中提取反复出现的规律与模式

形式化角度：如果算法利用某些经验，使自身在特定任务类上的性能得到改善，则认为该算法实现了人工智能

方法论角度：机器学习是 **计算机基于数据构建概率统计模型**，并 **运用该模型对数据进行预测与分析** 的学科

### 1.1.1  目标

机器学习的一个假设：已有的数据具有一定的统计特性，可以视为满足 **独立同分布** 的样本。

机器学习目标：根据已有的训练数据推导出所有数据的模型，并根据得出的模型实现对未知测试数据的最优预测

### 1.1.2 术语

数据（实例）：对对象某些性质的描述，不同的属性值有序排列得到的向量

- 属性：被描述的性质
- 属性值：属性的取值

不同的属性之间视为相互独立，每个属性都代表了不同的维度，这些属性共同张成了 **特征空间**

每个实例（数据）都可以看做特征空间中的一个向量—— **特征向量**

### 1.1.3 分类

根据输入输出类型：

- 回归问题：输入变量与输出变量均为连续变量
- 分类问题：输出变量为有限个离散变量，SP：二分类问题
- 标注问题：输入变量和输出变量均为变量序列

---

根据训练数据是否有标签信息：

- 监督学习：基于已知类别的训练数据进行学习
- 无监督学习：基于未知类别的训练数据进行学习
- 半监督学习：同时使用已知类别和未知类别的数据进行学习

### 1.1.4 模型精确性

#### 误差

> 学习器的预测输出与样本真实输出之间的差异被定义为机器学习中的误差

分类问题中，$误差率=\frac{分类错的样本数}{全部样本数}\times 100\%$ 

**训练误差** ：学习器在训练集上的误差，经验误差

- 描述输入属性和输出分类之间的相关性，能够判定给定的问题是不是一个容易学习的问题

**测测误差** ：学习器在测试集上的误差，泛化误差

- 反映了学习器对未知测试数据集的预测能力

实用的学习器都是测试误差较低，即在新样本上变现比较好的学习器

#### 噪声数据

训练样本本身还可能包含一些噪声，这些随机噪声会给模型精确性带来误差

#### 过拟合与欠拟合

过拟合：学习模型时包含的参数过多，从而导致训练误差较低但测试误差较高

- 表现为错把训练数据的特征当做整体的特征

欠拟合：学习能力太弱，以致于训练数据的基本性质都没学到

在实际的机器学习中，欠拟合可以通过改进学习器的算法克服，但过拟合却无法避免

- 由于训练样本的数量有限，所以具备有限个参数的模型就足以将所有样本都纳入其中。

  但模型的参数越多，与这个模型精确符合的数据也越少，将这样的模型运用到无穷的未知数据中，过拟合的出现便不可避免

#### 模型复杂度与测试误差

当模型复杂度较低时，测试误差较高

随着模型复杂度增加，测试误差将逐渐下降并达到最小值

之后当模型复杂度继续上升，测试误差会随之增加，对应过拟合的发生

#### 交叉验证

为了对 **测试误差** 做出更加精确的估计，一种广泛使用的方法是交叉验证

思想：重复利用有限训练样本，将数据分为若干子集，让不同子集分别组成训练集与测试集，并在此基础上反复训练，测试和模型选择，达到最优效果

---

若将训练数据分为10个子集 $D_{1-10}$ 进行交叉验证，则需要对模型进行10轮训练

- 第一轮训练集 $D_{2-10}$ ，训练出的学习器在 $D_1$ 上进行测试
- 第二轮训练集 $D_1,D_{3-10}$ ，训练出的学习器在 $D_2$ 上进行测试

当模型在10个子集全部完成测试后，其性能就是10次测试结果的均值

---

不同模型中平均测试误差最小的模型也就是最优模型

### 1.1.5 模型性能

参数的取值是影响模型性能的重要因素，同样的学习算法在不同的参数配置下，得到的模型性能会有显著差异

假设一个神经网络有1000个参数，每个参数有10种取值可能，对于每一组训练/测试集就有 $1000^{10}$ 个模型需要考察，因此在调参过程中，主要的问题就是性能与效率的折衷

## 监督学习

监督学习假定训练数据满足独立同分布，并根据训练数据学习出一个由输入到输出的映射模型

所有可能的映射模型共同构成了假设空间

监督学习的任务是在假设空间中根据特定的误差准则找到最优的模型

#### 分类

根据学习方法不同，分为生成方法与判别方法两类

**生成方法**：根据输入数据与输出数据之间的联合概率分布确定条件概率分布 $P(Y\vert X)$ 

- 生成方法表示了输入 $X$ 与输出 $Y$ 之间的生成关系

**判别方法**：直接学习条件概率分布 $P(Y\vert X)$ 或决策函数 $f(X)$ 

- 判别方法表示了根据 $X$ 得出输出 $Y$ 的预测方法

---

两种方法相较而言，生成方法具有更快的收敛速度和更广的应用范围；判别方法具有更高的准确率和更简单的使用方式



## 频率派与贝叶斯派

#### 频率派

频率本身会随机波动，但随着重复实验的次数不断增加，特定事件出现的频率值会呈现出稳定性，逐渐趋近于某个常数

从事件发生的频率认识概率的方法称为 “频率学派”。概率被认为是一个独立可重复实验中，单个结果出现频率极限。

**稳定的频率是统计规律性的体现** ，用其表征事件发生的可能性是一种合理的思路

频率学派依赖的是古典概型。由于古典概型只描述单个随机事件，并不能刻画两个随机事件之间的关系。所以引入的 **条件概率** ，进一步得出 **全概率公式** 。
$$
P(A)=\sum_\limits{i=1}^nP(A\vert B_i)\cdot P(B_i)
$$
全概率公式代表了频率派解决问题的思路：先做出一些假设 $P(B_i)$ ，再在这些假设下讨论随机事件的概率 $P(A\vert B_i)$ 

#### 贝叶斯派

**逆概率** ：由全概率公式调整得来，即在事件结果 $P(A)$ 确定的条件下，推断各种假设发生的可能性

通过贝叶斯公式，可以将后验概率 $P(D\vert H)$ 转变为先验概率  $P(H)$
$$
P(H\vert D)=\frac{P(D\vert H)P(H)}{P(D)}
$$

- $P(H)$ ：先验概率，假设成立的概率
- $P(D\vert H)$ ：似然概率
- $P(H\vert D)$ ：后验概率，已知结果下情况下假设成立的概率

贝叶斯定理提供了解决问题的新思路：根据观测结果寻找最佳的理论解释

### 0.3.1 区别

**频率学派** 认为假设是客观存在且不会改变的，即存在固定的先验分布，需要通过 **最大似然估计** 确定概率分布的类型和参数，以此作为基础进行概率推演。

**贝叶斯学派** 认为固定的先验分布是不存在的，即参数本身是随机数。假设本身取决于结果，是不确定的、可以修正的。数据的作用就是对假设不断修正，通过 **贝叶斯估计** 使后验概率最大化 。 

----

从 **参数估计** 角度也能体现两种思想的差距

由于实际任务中可供使用的训练数据有限，因而需要对概率分布的参数进行估计。

最大似然估计（最大似然概率 $P(D\vert H)$）的思想是使训练数据出现的概率最大化，以此确定概率分布中的未知参数

贝叶斯方法（最大后验概率 $P(H\vert D)$）：根据训练数据和已知的其他条件，使未知参数出现的可能性最大化，并选取最大概率对应的未知参数

- 还需要额外的信息 ——先验概率 $P(H)$







