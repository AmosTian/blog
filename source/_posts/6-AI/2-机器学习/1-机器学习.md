---
categories:
  - AI
  - 机器学习
tags:
  - AI
  - 机器学习
top: 8
mathjax: true
title: 1. 机器学习绪论
date: 2023-8-21 15:27:44
---

[TOC]

<!--more-->

## 1.1 机器学习概念

人类学习机制：从大量现象中提取反复出现的规律与模式

形式化角度：如果算法利用某些经验，使自身在特定任务类上的性能得到改善，则认为该算法实现了人工智能

方法论角度：机器学习是 **计算机基于数据构建概率统计模型**，并 **运用该模型对数据进行预测与分析** 的学科

### 1.1.1  目标

机器学习的一个假设：已有的数据具有一定的统计特性，可以视为满足 **独立同分布** 的样本。

机器学习目标：根据已有的训练数据推导出所有数据的模型，并根据得出的模型实现对未知测试数据的最优预测

### 1.1.2 术语

数据（实例）：对对象某些性质的描述，不同的属性值有序排列得到的向量

- 属性：被描述的性质
- 属性值：属性的取值

不同的属性之间视为相互独立，每个属性都代表了不同的维度，这些属性共同张成了 **特征空间**

每个实例（数据）都可以看做特征空间中的一个向量—— **特征向量**

### 1.1.3 分类

根据输入输出类型：

- 回归问题：输入变量与输出变量均为连续变量
- 分类问题：输出变量为有限个离散变量，SP：二分类问题
- 标注问题：输入变量和输出变量均为变量序列

---

根据训练数据是否有标签信息：

- 监督学习：基于已知类别的训练数据进行学习
- 无监督学习：基于未知类别的训练数据进行学习
- 半监督学习：同时使用已知类别和未知类别的数据进行学习

### 1.1.4 模型精确性

#### 误差

> 学习器的预测输出与样本真实输出之间的差异被定义为机器学习中的误差

分类问题中，$误差率=\frac{分类错的样本数}{全部样本数}\times 100\%$ 

**训练误差** ：学习器在训练集上的误差，经验误差

- 描述输入属性和输出分类之间的相关性，能够判定给定的问题是不是一个容易学习的问题

**测测误差** ：学习器在测试集上的误差，泛化误差

- 反映了学习器对未知测试数据集的预测能力

实用的学习器都是测试误差较低，即在新样本上变现比较好的学习器

#### 噪声数据

训练样本本身还可能包含一些噪声，这些随机噪声会给模型精确性带来误差

#### 过拟合与欠拟合

过拟合：学习模型时包含的参数过多，从而导致训练误差较低但测试误差较高

- 表现为错把训练数据的特征当做整体的特征

欠拟合：学习能力太弱，以致于训练数据的基本性质都没学到

在实际的机器学习中，欠拟合可以通过改进学习器的算法克服，但过拟合却无法避免

- 由于训练样本的数量有限，所以具备有限个参数的模型就足以将所有样本都纳入其中。

  但模型的参数越多，与这个模型精确符合的数据也越少，将这样的模型运用到无穷的未知数据中，过拟合的出现便不可避免

#### 模型复杂度与测试误差

当模型复杂度较低时，测试误差较高

随着模型复杂度增加，测试误差将逐渐下降并达到最小值

之后当模型复杂度继续上升，测试误差会随之增加，对应过拟合的发生

#### 交叉验证

为了对 **测试误差** 做出更加精确的估计，一种广泛使用的方法是交叉验证

思想：重复利用有限训练样本，将数据分为若干子集，让不同子集分别组成训练集与测试集，并在此基础上反复训练，测试和模型选择，达到最优效果

---

若将训练数据分为10个子集 $D_{1-10}$ 进行交叉验证，则需要对模型进行10轮训练

- 第一轮训练集 $D_{2-10}$ ，训练出的学习器在 $D_1$ 上进行测试
- 第二轮训练集 $D_1,D_{3-10}$ ，训练出的学习器在 $D_2$ 上进行测试

当模型在10个子集全部完成测试后，其性能就是10次测试结果的均值

---

不同模型中平均测试误差最小的模型也就是最优模型

### 1.1.5 模型性能

参数的取值是影响模型性能的重要因素，同样的学习算法在不同的参数配置下，得到的模型性能会有显著差异

假设一个神经网络有1000个参数，每个参数有10种取值可能，对于每一组训练/测试集就有 $1000^{10}$ 个模型需要考察，因此在调参过程中，主要的问题就是性能与效率的折衷

## 监督学习

监督学习假定训练数据满足独立同分布，并根据训练数据学习出一个由输入到输出的映射模型

所有可能的映射模型共同构成了假设空间

监督学习的任务是在假设空间中根据特定的误差准则找到最优的模型

#### 分类

根据学习方法不同，分为生成方法与判别方法两类

**生成方法**：根据输入数据与输出数据之间的联合概率分布确定条件概率分布 $P(Y\vert X)$ 

- 生成方法表示了输入 $X$ 与输出 $Y$ 之间的生成关系

**判别方法**：直接学习条件概率分布 $P(Y\vert X)$ 或决策函数 $f(X)$ 

- 判别方法表示了根据 $X$ 得出输出 $Y$ 的预测方法

---

两种方法相较而言，生成方法具有更快的收敛速度和更广的应用范围；判别方法具有更高的准确率和更简单的使用方式
