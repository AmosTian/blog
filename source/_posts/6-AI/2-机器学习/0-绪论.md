---
0000categories:
  - AI
tags:
  - AI
top: 7
mathjax: true
title: 0.人工智能概述
date: 2023-8-17 09:33:58
---

> 参考：
>
> [极客时间——人工智能基础课](http://gk.link/a/128vk)

<!--more-->

人工智能本质上是一种劳动工具，但当劳动工具本身强大到反客为主时，作为劳动者的人类便成为了多余的角色，有降格为“亚人工智能”的风险。解决方案：

- 专精于依赖创造力的领域，如科学与艺术，但对天赋要求极高，不适合每一个人
- 掌握核心技术，让人工智能回归 "为我所用"  的工具性

## 0.1 概念

人工智能建立在以线性代数和概率论为骨架的基础数学工程上，通过简单模型的组合时限复杂功能。

工程上，深度神经网络通常以其大量的参数让人望而却步，可在理论上，其数学原理却又更好的可解释性 

### 0.1.1 发展

人工智能的早期发展遵循的是符号主义学派的发展路径，但狭窄的应用领域让它在短暂的辉煌之后迅速走向沉寂。

连接主义通过以工程技术手段模拟人脑神经系统的结构和功能来模拟人脑形象思维的能力，也成为今天人工智能的核心技术。

## 0.2 学习模块

### 0.2.1 数学

- [x] 高等数学
- [x] 线性代数
- [x] 概率论
- [x] 最优化方法
- [x] 信息论
- [x] 形式逻辑

### 0.2.2 机器学习

**机器学习的作用是从数据中习得学习算法，进而解决实际的应用问题** 

- 线性回归
- 决策树
- 支持向量机
- 聚类

### 0.2.3 人工神经网络

神经网络将认知科学引入机器学习当中，以模拟生物神经系统对真实世界的交互反应，取得了良好的效果

- 多层神经网络
- 前馈与反向传播
- 自组织神经网络

### 0.2.4 深度学习

深度学习就是包含多个中间层的神经网络

关键因素：数据爆炸+算力提升

- 深度前馈网络
- 深度学习中的正则化
- 自动编码器

### 0.2.5 神经网络实例

深度学习框架下，一些神经网络已经用于各种应用场景，并取得了不俗的效果。

- 卷积神经网络
- 递归神经网络
- 深度信念网络

### 0.2.6 深度学习之外的人工智能

- 马尔可夫随机场
- 迁移学习
- 集群智能

### 0.2.7 应用实例

除代替人类执行重复性的劳动，在诸多实际问题中，人工智能也提供了有意义的尝试

- 计算机视觉
- 语音识别
- 对话系统

## 0.3 频率派与贝叶斯派

#### 频率派

频率本身会随机波动，但随着重复实验的次数不断增加，特定事件出现的频率值会呈现出稳定性，逐渐趋近于某个常数

从事件发生的频率认识概率的方法称为 “频率学派”。概率被认为是一个独立可重复实验中，单个结果出现频率极限。

**稳定的频率是统计规律性的体现** ，用其表征事件发生的可能性是一种合理的思路

频率学派依赖的是古典概型。由于古典概型只描述单个随机事件，并不能刻画两个随机事件之间的关系。所以引入的 **条件概率** ，进一步得出 **全概率公式** 。
$$
P(A)=\sum_\limits{i=1}^nP(A\vert B_i)\cdot P(B_i)
$$
全概率公式代表了频率派解决问题的思路：先做出一些假设 $P(B_i)$ ，再在这些假设下讨论随机事件的概率 $P(A\vert B_i)$ 

#### 贝叶斯派

**逆概率** ：由全概率公式调整得来，即在事件结果 $P(A)$ 确定的条件下，推断各种假设发生的可能性

通过贝叶斯公式，可以将后验概率 $P(D\vert H)$ 转变为先验概率  $P(H)$
$$
P(H\vert D)=\frac{P(D\vert H)P(H)}{P(D)}
$$

- $P(H)$ ：先验概率，假设成立的概率
- $P(D\vert H)$ ：似然概率
- $P(H\vert D)$ ：后验概率，已知结果下情况下假设成立的概率

贝叶斯定理提供了解决问题的新思路：根据观测结果寻找最佳的理论解释

### 0.3.1 区别

**频率学派** 认为假设是客观存在且不会改变的，即存在固定的先验分布，需要通过 **最大似然估计** 确定概率分布的类型和参数，以此作为基础进行概率推演。

**贝叶斯学派** 认为固定的先验分布是不存在的，即参数本身是随机数。假设本身取决于结果，是不确定的、可以修正的。数据的作用就是对假设不断修正，通过 **贝叶斯估计** 使后验概率最大化 。 

----

从 **参数估计** 角度也能体现两种思想的差距

由于实际任务中可供使用的训练数据有限，因而需要对概率分布的参数进行估计。

最大似然估计（最大似然概率 $P(D\vert H)$）的思想是使训练数据出现的概率最大化，以此确定概率分布中的未知参数

贝叶斯方法（最大后验概率 $P(H\vert D)$）：根据训练数据和已知的其他条件，使未知参数出现的可能性最大化，并选取最大概率对应的未知参数

- 还需要额外的信息 ——先验概率 $P(H)$









