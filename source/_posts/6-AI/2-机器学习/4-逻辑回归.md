---
categories:
  - AI
  - 机器学习
tags:
  - AI
  - 机器学习
top: 10
mathjax: true
title: 4. 逻辑回归
date: 2023-8-22 15:53:36
---

> 用判别模型解决分类问题

[TOC]

<!--more-->

## 逻辑回归介绍

逻辑回归源于对线性回归算法的改进

通过引入单调可微函数 $g(\cdot)$ ，线性回归模型可以推广为 $y=g^{-1}(w^Tx)$ ，进而将线性回归模型预测值与分类任务的离散标记联系起来。当 $g(\cdot)$ 取成对数形式时，线性回归就演变成了逻辑回归

### 对数几率函数

在简单的 **二分类问题** 中，分类的标记可以抽象为0和1，因而线性回归中的实值输出需要映射为二进制的结果。

逻辑回归中，实现这一映射的是对数几率函数，也即 **sigmod函数**
$$
y=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-w^Tx}}
$$

- 对数几率函数能够将线性回归从负无穷到正无穷的输出范围压缩到 $(0,1)$ 之间

- 当线性回归结果是 $z=0$ 时，逻辑回归的结果是 $y=0.5$ ，这可以是做一个分界点

  当 $z>0$ ，则 $y>0$ ，此时逻辑回归的结果就被判为正例

  当 $z<0$ ，则 $y<0$ ，此时逻辑回归的结果就被判为负例

如果将对数几率函数的结果 $y$ 视为样本 $x$ 作为正例的可能性，则 $1-y$ 就是其作为反例的可能性，两者比值 $0<\frac{y}{1-y}<\infty$ ，称为 **几率** 。对 **几率** 取对数，可得到

$$
ln\frac{y}{1-y}=w^Tx
$$
可见，当利用逻辑回归模型解决分类任务时，线性回归的结果正是以对数几率的形式出现的
$$
P(Y=1\vert X)=\frac{1}{1+e^{-w^TX}}=\frac{e^{w^TX}}{1+e^{w^TX}}\\
P(Y=0\vert X)=\frac{e^{-w^TX}}{1+e^{-w^TX}}=\frac{1}{1+e^{w^TX}}
$$
对于给定的实例，逻辑回归模型比较两个条件概率值的大小，并将实例划分到概率较大的分类中

### 逻辑回归模型参数求解

假设样本独立同分布

学习时，逻辑回归模型在给定的训练数据集上应用 **最大似然估计法** 确定模型的参数

- 对于给定的数据集 $(X_i,y_i)$ ，逻辑回归**使每个样本属于其真实标记的概率最大化**，以此为依据确定 $w$ 的最优值

似然函数可表示为
$$
L(w\vert X)=P(\omega\vert X)\xlongequal{x_i独立同分布}\prod_\limits{i=1}^n[P(Y=1\vert x_i,w)]^{y_i}[1-P(Y=1\vert x_i,w)]^{1-y_i}
$$

取对数后简化运算
$$
\ln L(w\vert X)=\sum_\limits{i=1}^n\left\{y_i\ln P(Y=1\vert x_i,w)+(1-y_i)\ln \left[1-P(Y=1\vert x_i,w)\right]\right\}
$$
由于单个样本的标记 $y_i$ 只能取得0或1，因而上式中的两项中只有一个非零值，代入对数几率，经过化简后可以得到
$$
\ln L(w\vert X)=\sum_\limits{i=1}^n\left[y_i(w^Tx_i)-\ln(1+e^{w^Tx_i})\right]
$$
寻找上述函数的最大值就是以对数似然函数为目标函数的最优化问题，通常使用 **梯度下降法** 或 **牛顿法** 求解

### 多角度分析逻辑回归

#### 信息论角度

当训练数据集是从所有数据中均匀抽取且数据量较大时，可以**从信息论角度**解释：

对数似然函数的最大化可以等效为待求模型与最大熵模型之间的KL散度的最小化。即逻辑回归对参数做出的额外假设是最少的。

#### 数学角度

从**数学角度**看，线性回归与逻辑回归之间的关系在于非线性的对数似然函数

- 从特征空间看，两者的区别在于数据判断边界上的变化

  利用回归模型只能得到线性的判定边界；

  逻辑回归则在线性回归的基础上，通过对数似然函数的引入使判定边界的形状不再受限于直线

#### 与朴素贝叶斯对比

**联系**

逻辑回归与朴素贝叶斯都是利用条件概率 $P(Y\vert X)$ 完成分类任务，二者在特定条件下可以等效。

- 用朴素贝叶斯处理二分类任务时，假设对每个属性 $x_i$ ，属性条件概率 $P(X=x_i\vert Y=y_k)$ 都满足正态分布，且正态分布的标准差与输出标记 $Y$ 无关，则根据贝叶斯定理，后验概率可写成 
  $$
  P(Y=0\vert X)=\frac{P(Y=0)\cdot P(X\vert Y=0)}{P(Y=1)\cdot P(X\vert Y=1)+P(Y=0)\cdot P(X\vert Y=0)}\\
  =\frac{1}{1+e^{ln\frac{P(Y=1)\cdot P(X\vert Y=1)}{P(Y=0)\cdot P(X\vert Y=0)}}}
  $$
   在条件独立性假设的前提下，类条件概率可以表示为属性条件概率的乘积。令先验概率 $P(Y=0)=p_0$ 并将满足正态分布的属性条件概率 $P(X^{(i)}\vert Y=y_k)$ 代入，可得
  $$
  P(Y=0\vert X)=\frac{1}{1+e^{ln\frac{1-p_0}{p_0}+\sum_\limits{i}\left(\frac{\mu_{i1}-\mu_{i0}}{\sigma^2}X_i+\frac{\mu_{i0}^2-\mu_{i1}^2}{2\sigma^2_i}\right)}}
  $$
  可见，上式的形式和逻辑回归中条件概率 $P(Y=0\vert X)$ 是完全一致的，**朴素贝叶斯学习器和逻辑回归模型学习到的是同一个模型**

**区别**

属于不同的监督学习类型

- 朴素贝叶斯是生成模型的代表：先由训练数据估计出输入和输出的联合概率分布，再根据联合概率分布来生成符合条件的输出，$P(Y\vert X)$ 以后验概率的形式出现
- 逻辑回归是判别模型的代表：先由训练数据集估计出输入和输出的条件概率分布，再根据条件概率分布判定对于给定的输入应该选择哪种输出，$P(Y\vert X)$ 以似然概率的形式出现

当朴素贝叶斯学习器的条件独立性假设不成立时，逻辑回归与朴素贝叶斯方法通常会学习到不同的结果

- 当训练样本数接近无穷大时，逻辑回归的渐进分类准确率要优于朴素贝叶斯方法

  逻辑回归不依赖于条件独立性假设，偏差更小，但方差更大

- 当训练样本稀疏时，朴素贝叶斯的性能优于逻辑回归

收敛速度不同：逻辑回归的收敛速度慢于朴素贝叶斯方法

## 从二分类到多分类问题

要让逻辑回归处理多分类问题，需要做一些改进

### 多次二分类

通过多次二分类实现多个类别的标记

- 等效为直接将逻辑回归应用在每个类别上，对每个类别建立一个二分类器

  如果输出的类别标记数为 $m$ ，就可以得到 $m$ 个针对不同标记的二分类逻辑回归模型，对一个实例的分类结果就是这 $m$ 个分类函数中输出值最大的那个

- 对一个实例执行分类需要多次使用逻辑回归算法，效率低下

### softmax

直接修改逻辑回归的似然概率，使之适应多分类问题——softmax回归

softmax给出的是实例在每一种分类结果下出现的概率
$$
P(Y=k\vert X)=\frac{e^{w_k^TX}}{\sum_\limits{k=1}^K e^{w_k^TX}}
$$

- $w_k$ 表示和类别 $k$ 相关的权重参数

Softmax回归模型的训练与逻辑回归模型类似，都可以转化为通过梯度下降法或者拟牛顿法解决最优化问题

### 两种方式对比

输出结果只能是属于一个类别时，Softmax分类器更加高效

输出结果可能出现交叉情况时，多个二分类逻辑回归模型性能好，可以得到多个类别的标记
