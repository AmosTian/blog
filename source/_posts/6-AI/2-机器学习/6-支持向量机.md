---
categories:
  - AI
  - 机器学习
tags:
  - AI
  - 机器学习
top: 12
mathjax: true
title: 6. 支持向量机
date: 2023-8-24 16:48:47
---

[TOC]

<!--more-->

支持向量机是一种二分类算法，通过在高维空间中构建超平面实现对样本的分类

## 线性可分SVM

eg：SVM解决二分类问题的思路

线性可分的数据集可以简化为二维平面上的点集。

在直角坐标系中，如果有若干个点位于 $x$ 轴下方，另外若干个点位于 $x$ 轴上方，这两个点集共同构成了一个线性可分的训练数据集，而 $x$ 轴就是将它们区别开的一维超平面，也就是直线。

假设 $x$ 轴上方的点全部位于直线 $y=1$ 上及其上方，$x$ 轴下方的点全部位于直线 $y=-2$ 上及其下方。则任何平行于 $x$ 轴且在 $(-2,1)$ 之间的直线都可以将这个训练集分开。但此时面临选择哪一条直线分类效果最好的问题。

直观上看 $y=-0.5$ 最好，这条分界线正好位于两个边界中间，**与两个类别的间隔** 可以同时达到最大。当训练集中的数据因为噪音干扰而移动时，这个最优划分超平面的划分精确度受到的影响最小，具有很强的泛化能力

### 线性可分SVM基本思想

在高维的特征空间上，划分超平面可以用简单的线性方程描述 
$$
\omega^T\cdot X+b=0
$$

- $n$ 维向量 $\omega$ 为法向量，决定了超平面的方向
- $b$ 为截距，决定了超平面与高维空间中原点的距离

划分超平面将特征空间分为两部分

- 法向量所指一侧——正例，$y=+1$
- 法向量反方向恻——负例，$y=-1$

**线性可分支持向量机就是在给定训练数据集的条件下，根据硬间隔最大化学习最优的划分超平面过程**

给定超平面后，特征空间中的样本点 $X_i$ 到超平面的距离可以表示为
$$
\gamma=\frac{\omega^T\cdot X_i+b}{\Vert \omega\Vert}
$$
这个距离是一个归一化距离——**几何间隔**

通过合理设置参数 $\omega$ 和 $b$ ，可以使每个样本到达最优划分超平面的距离都不小于 $1$ ，即有 **函数间隔**
$$
\omega^T\cdot X_i+b\ge 1,y_i=+1\\
\omega^T\cdot X_i+b\le -1,y_i=-1\\
$$

- 函数间隔与几何将的区别在于未归一化和归一化

在特征空间中，距离划分超平面最近的样本点能让函数函数间隔取等号，这些样本点称为 **支持向量** ，即有点 $X_{k^+},X_{k^-}$ 
$$
\omega^T\cdot X_{k^+}+b= 1,y_{k^+}=+1\\
\omega^T\cdot X_{k^-}+b= -1,y_{k^-}=-1\\
$$
两个异类支持向量到超平面的距离之和为 $\frac{2}{\Vert \omega\Vert}$ 。

因而对于线性可分的SVM任务就是：在满足上述不等式的条件下，寻找 $\frac{2}{\Vert \omega\Vert}$ 的最大值，最大化 $\frac{2}{\Vert \omega\Vert}$ 等效于最小化 $\frac{1}{2}\Vert \omega\Vert^2$  

### 线性SVM基本思想

当训练集线性不可分时，学习策略变为 **软间隔最大化**

线性不可分意味着某些样本点距离划分超平面的函数间隔不满足不小于1的约束条件，因而需要对每个样本点引入大于零的松弛变量 $\xi\ge 0$ ，使得函数间隔和松弛变量的和不小于1，即约束条件为
$$
\omega^T\cdot X_i+b\ge 1-\xi_i,y_i=+1\\
\omega^T\cdot X_i+b\le-1+\xi_i,y_i=-1\\
$$
相应地，最优化问题中的目标函数演变为 $\frac{1}{2}\Vert \omega\Vert^2+C\sum_\limits{i=1}^n\xi^2_i$ ，其中 $C$ 被称为 **惩罚参数**

软间隔允许某些样本不满足应间隔的约束，但会限制这些特例的数目







