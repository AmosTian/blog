---
categories:
  - AI
  - 机器学习
tags:
  - AI
  - 机器学习
top: 9
mathjax: true
title: 3. 朴素贝叶斯
abbrlink: 2792113303
date: 2023-08-22 09:57:01
---

[TOC]

> 用于解决分类问题：将连续取值输入映射为离散取值的输出
>
> 解决分类问题的依据是数据的属性
>

- 利用后验概率选择最佳分类，后验概率通过贝叶斯定理求解
- 朴素贝叶斯假定所有属性相互独立，基于这一假设将类条件概率转化为属性条件概率的乘积
- 朴素贝叶斯方法可以使期望风险最小化
- 影响朴素贝叶斯分类的是所有属性之间的依赖关系在不同类别上的分布

<!--more-->

## 朴素贝叶斯分类器介绍

### 朴素贝叶斯分类器思想

**朴素贝叶斯分类器假定样本的不同属性满足条件独立性假设**

其基本思想：分析待分类样本出现在每个输出类别的后验概率 $P(Y\vert X)$ ，并将取得最大后验概率的类别作为输出

假设训练数据的属性由 $n$ 维随机变量 $X$ 表示，其分类结果用随机变量 $Y$ 表示，那么 $X$ 和 $Y$ 的统计规律就可以用联合概率分布 $P(X,Y)$ 表示，每一个具体的样本 $(x_i,y_i)$ 都可以由 $P(X,Y)$ 独立同分布生成——**生成学习**
$$
P(Y\vert X)=\frac{P(X,Y)}{P(X)}=\frac{P(Y)\cdot P(X\vert Y)}{P(X)}
$$
$P(Y)$ 表示每个类别出现的概率，也就是类 **先验概率**

- 先验概率容易根据训练数据计算出来，只需要分别统计不同类别样本的数量即可

$P(X\vert Y)$ 表示在给定的类别下不同属性出现的概率，即类**似然概率**

- 似然概率受属性值的影响

### 条件独立性对似然概率计算的影响

如果每个样本包含 100 个属性，每个属性取值可能有 100 种，那么对分类的每个结果，要计算的条件概率数目就是 $100^{100}$ ，对似然概率的精确估计就需要庞大的计算量

在条件独立性假设的前提下，保证了所有属性相互独立，互不影响。每个属性独立地对分类结果发生作用，即
$$
P(X=\vec{x}\vert Y=c)=P\left(X^{(1)}=x_1,X^{(2)}=x_2,\cdots,X^{(m)}=x_m\vert Y=c\right)\\
\xlongequal{所有属性相互独立}\prod_\limits{j=1}^mP(X^{(j)}=x_j^{(j)}\vert Y=c)
$$
**在条件独立性假设下，将类条件概率转化为属性条件概率的乘积**

在没有条件独立性假设的情况下，每个样本分类结果 $y$ 只能刻画所有属性 $X^{(1)},X^{(2)},\cdots,X^{(m)}$ 形成的整体，且只有具有相同属性的样本才能放在一起评价

- 当属性数目较多而样本数目较少时，要让 $m$ 个属性取到相同特征就有些牵强

有了条件独立性假设后，分类结果 $y$ 就相当于实现了 $m$ 重复用。每个样本既可以用于刻画 $X^{(1)}$ ，又可以刻画 $X^{(n)}$ ，

- 无形中将训练样本的数量扩大为原先的 $m$ 倍

分析每个属性取值对分类结果的影响时，也有更多的数据作为支撑

---

条件独立性假设是一个很强的假设，导致对数据的过度简化，因而对性能带来些许影响。但由于其极大简化分类问题计算复杂度的能力，性能上做部分折衷也并非不能接受

### 朴素贝叶斯分类器

有了训练数据，先验概率 $P(Y)$ 和似然概率 $P(X\vert Y)$ 就可被视为已知条件，进而可用于求解后验概率 $P(Y\vert X)$ 。

对于给定的输入 $X$ ，朴素贝叶斯分类器就可以利用贝叶斯定理求解后验概率，并将后验概率最大的类作为输出

由于所有的后验概率求解中，边界概率 $P(X)$ 都是相同的，因而其影响可忽略，有朴素贝叶斯分类器的数学表达式
$$
y=arg\max_\limits{c_k}P(Y=c_k)\cdot \prod_\limits{j}P(X^{(j)}=x_j\vert Y=c_k)
$$

---

朴素贝叶斯是一种非常高效的方法。当以分类的正确与否作为误差指标时，只要朴素贝叶斯分类器能够把最大的后验概率找到，就意味着分类正确。至于最大后验概率的估计值是否精确，就不重要了

- 对于一个2分类问题，在一个实例上两个类别的最大厚颜概率分别是 0.9和0.1，朴素贝叶斯分类器估计出的后验概率就可能是0.6和0.4。由于大小相对关系没有改变，按照估计的后验概率分类，仍然能得到正确的结果

## 朴素贝叶斯分类器分析

### 条件独立性假设分析

如何解释独立性假设在几乎不成立的情况下，朴素贝叶斯分类器在大多数分类任务中体现出优良特性？

**影响朴素贝叶斯的分类的是所有属性之间的依赖关系在不同类别上的分布，而不是依赖关系本身**

- 在给定训练数据集上，两个属性之间可能具有相关性，但在每个类别上都以相同的程度体现，这种情况下不会破坏贝叶斯分类器的最优性
- 即使这种分布式不均匀的，当所有属性之间的依赖关系一起发挥作用时，他们就可能相互抵消，不再次影响分类

### 期望风险最小化

在应用朴素贝叶斯分类器处理连续性属性数据时，通常假定属性数据满足正态分布，再根据每个类别下的训练数据计算出正态分布的均值和方差

从模型最优化角度观察，朴素贝叶斯分类器是平均意义上预测能力最优的模型，也就是使得 **期望风险最小化**

- 期望风险：风险函数的数学期望，度量平均意义下模型预测的误差特性。

  可视为单次预测误差在联合概率分布 $P(X,Y)$ 上的数学期望

**期望风险最小化 $\iff$ 后验概率最大化**

朴素贝叶斯分类器通过将实例分配到后验概率最大的类中，也就是让 $1-P(Y\vert X)$ 最小。

在以分类错误的是  实例数作为误差时，期望风险就等于 $1-P(Y\vert X)$ 

### 拉普拉斯平滑

为了避免属性携带的信息被训练过程中未出现的属性值所干扰，在计算属性条件概率时，添加一个 **拉普拉斯平滑** 的步骤

- 受到训练集规模的限制，某些属性的取值训练集中可能从未与某个类同时出现，导致属性的条件概率为0，进而产生错误的结论

## 半朴素贝叶斯分类器

考虑了部分属性之间的依赖关系，既保留了属性之间较强的相关性，又不需要完全计算复杂的联合概率分布

常用的方法是建立独依赖关系：假设每个属性除了类别之外，最多只依赖一个其他属性















