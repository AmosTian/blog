---
categories:
  - AI
  - 机器学习
tags:
  - AI
  - 机器学习
top: 8
mathjax: true
title: 2. 线性回归
abbrlink: 1027589108
date: 2023-08-21 20:00:32
---

> 线性回归既能体现出重要的基本思想，又能构造出功能更加强大的非线性模型

[TOC]

---

线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合的最优系数

最小二乘法可用于解决单变量线性回归问题，当误差函数服从正态分布时，与最大似然估计等价

多元回归问题也可以用最小二乘法求解，但极易出现过拟合线性

- 岭回归，引入二范数惩罚项
- LASSO回归，引入一范数项

<!--more-->

## 线性回归概念

> 线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合中的最优系数

- 线性回归模型最易于拟合，其估计结果的统计特性也更容易确定
- 在机器学习中，回归问题隐含了输入变量与输出变量均可连续取指的前提，因而利用线性回归模型可以对任意输入给出输出的估计

### 回归模型历史

1875年，从事遗传问题研究的英国统计学家弗朗西斯·高尔顿正在寻找子代与父代身高之间的关系。

他发现数据散点图大致呈直线状态（父代身高与子代身高呈正相关关系）

高尔顿将这种现象称为 **回归效应** ，即大自然将人类身高的分布约束在相对稳定并不产生两极分化的整体水平，并给出了历史上第一个线性回归的表达式：$y=0.516x+33.73$

### 线性回归模型基本形式

假定一个实例可以用列向量 $X=\left(\begin{aligned}x_1\\x_2\\\vdots\\x_n\end{aligned}\right)$ 表示，每个 $x_i$ 代表实例在第 $i$ 个属性上的取值。

线性回归的作用就是习得一组参数 $w_i,i=0,1,\cdots,n$ ，使预测输出可以表示为以这组参数为权重的实例属性的线性组合，即
$$
f(X)=w^TX=\sum_\limits{i=0}^nw_i\cdot x_i
$$

- 当实例只有一个属性时，输入和输出之间的关系就是二维平面上的一条直线
- 当实例有 $n$ 个属性时，输入和输出之间的关系就是 $n+1$ 维空间上的一个超平面，对应一个维度为 $n$ 的线性子空间

## 线性回归的误差

在训练集上确定系数 $w_i$ 时，预测输出 $f(X)$ 和真实输出 $y$ 之间的误差以 **均方误差** 来定义

### 单变量线性回归模型

当线性回归的模型为二维平面上的直线时，均方误差就是 $f(X)$ 与 $y$ 之间的 **欧几里得距离** ，即两点之间的 $L_2范数$ 

使均方误差取得最小值为目标的模型求解方法为 **最小二乘法**
$$
w^*=arg \min_\limits{w}\sum_\limits{k=1}^n(w^Tx_k-y_k)^2=arg \max_\limits{w}\sum_\limits{k=1}^n\Vert w^Tx_k-y_k\Vert^2
$$
在单变量线性回归任务中，最小二乘法的作用就是找到一条直线，使 **所有样本到直线的欧氏距离之和最小**

#### 为什么使均方误差最小化的参数就是和训练样本匹配的最优模型

从 **概率论** 的角度解释，线性回归得到的是统计意义上的拟合结果，在单变量的情形下，可能一个样本点都没有落在求得的直线上

对上述现象的解释是：回归结果可以完美匹配理想样本点的分布，但训练中使用的真实样本点是理想样本点和噪声叠加的结果，因而与回归模型之间产生了偏差，每个样本点上噪声的取值等于 $y_k-f(x_k)$

假定样本点的噪声满足参数为 $(0,\sigma^2)$ 的正态分布，这意味着噪声等于0的概率密度最大。

在这种情况下，对参数 $w$ 的推导就可以用 **最大似然估计** 进行，即在已知样本数据及其分布的条件下，找到使样本数据以最大概率出现的参数假设 $w$

在假设每个样本独立同分布的前提下，似然概率写作
$$
P(x_1,x_2,\cdots,x_k\vert w)=\prod_\limits{k}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_k-w^Tx_k)^2}{2\sigma^2}}
$$
最大似然估计的任务就是让上述表达式的取值最大化。为便于计算，对似然概率取对数

$$
\ln P(x_1,x_2,\cdots,x_n\vert w)=-\sum_\limits{k}^n\left[\ln \sqrt{2\pi}\sigma+\frac{(y_k-w^Tx_k)^2}{2\sigma^2}\right]
$$
并经过运算后，似然概率的最大化等效为 $\sum_\limits{k=1}^n(w^Tx_k-y_k)^2$ 的最小化

---

在误差函数服从正泰分布的情况下，从几何意义出发的最小二乘法与从概率意义出发的最大似然估计是等价的

#### 求解

单变量线性回归模型的回归方程可写为 $y=w_1x+w_0$ .

根据最优化理论，将这一表达式代入均方差误差表达式中，即 $L(w_1,w_0)=\sum_\limits{k=1}^n(w_1x_k+w_0-y_k)^2$

分别对 $w_1$ 和 $w_0$ 求偏导，令两个偏导数均等于0的取值就是线性回归的最优解
$$
w_0=\frac{1}{n}\sum_\limits{k=1}^n(y_k-w_1x_k)\\
w_1=\frac{\sum_\limits{k=1}^ny_k(x_k-\frac{1}{n}\sum_\limits{k=1}^nx_k)}{\sum_\limits{k=1}^nx_k^2-\frac{1}{n}\left(\sum_\limits{k=1}^nx_k\right)^2}
$$

### 多元线性回归

多元线性回归中的参数 $w$ 也可用最小二乘法估计。求解过程仍是用偏导等于零确定，但参与运算的元素从向量编程矩阵。

多元参数下，多元线性回归的最优参数为 
$$
w^*=(X^TX)^{-1}X^Ty
$$
$X=\left(\begin{aligned}x_1\\x_2\\\vdots\\x_n\end{aligned}\right)$ 表示所有样本。这一表达式只在矩阵 $X^TX$ 存在的情况下成立

---

在大量复杂的实际任务中，每个样本属性的数目甚至会超过训练集中的样本总数，此时求出的 $w^*$ 不是唯一的，解的选择依赖于学习算法的归纳偏好

但无论怎样选择标准，存在多个最优解的问题不会改变，极易出现过拟合现象

#### 正则化解决过拟合问题

为解决过拟合问题，常见的做法是 **正则化** ，即添加额外的惩罚项。根据使用的惩罚项不同，分为

- 岭回归
- LASSO回归

其共同思想：通过惩罚项的引入抑制过拟合现象，以训练误差增加为代价换取测试误差下降

##### 岭回归

也称 **参数衰减**  

岭回归实现正则化的方式是在原始均方误差的基础上，加一个待求解参数的二范数项，即最小化求解的对象变为
$$
\Vert y_k-w^Tx_k\Vert^2+\Vert \Gamma w\Vert^2，\Gamma为季霍诺夫矩阵
$$

- 季霍诺夫矩阵主要目的是解决矩阵求逆的稳定性问题

##### LASSO回归

最小绝对缩减和选择算子

LASSO回归选择了待求解参数的一范数作为惩罚项，即最小化求解的对象变为 
$$
\Vert y_k-w^Tx_k\Vert^2+\lambda \Vert w\Vert_1
$$

##### 岭回归与LASSO回归概率角度

**从最优化角度**

岭回归：二范数惩罚项的作用在于优先选择范数较小的 $w$ 。相当于在最小均方误差之外额外添加了一重约束条件，将最优解限制在高维空间内的一个球内

- 在最小二乘的结果上做了缩放，虽然最优解中参数的贡献被削弱了，但参数的数目没有变少

LASSO回归：引入稀疏性，降低了最优解 $w$ 维度，使一部分参数的贡献度 $w_i=0$ ，使得 $w$ 中元素数目大大小于原始特征的数目

- 引入稀疏性是简化复杂问题的一种常用方法，在数据压缩，信号处理等领域亦有应用

**从概率角度看**

岭回归是在 $w_i$ 满足正态先验分布的条件下，用最大后验概率进行估计得到的结果

LASSO回归是在 $w_i$ 满足拉普拉斯先验分布的条件下，用最大后验概率进行估计得到的结果









