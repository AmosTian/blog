---
title: 人工智能-数学基础
categories:
  - AI
  - 人工智能
tags:
  - AI
  - 人工智能
mathjax: true
abbrlink: 4279949973
date: 2022-11-01 13:45:51
---

> 线性代数，概率论，数理统计，最优化方法，信息论，形式逻辑

<!--more-->

# 2. 数学基础

## 2.1 liner algebra

> 线性代数的意义在于提供了一种看待世界的抽象角度
>
> 万物都可被抽象为某些特征的组合，并在预置规则定义的框架之下以静态或动态的方式加以观察

### 2.1.1 集合

> 线性代数中最基本的概念是 **set**

集合的定义是由某些具有某些共性的对象汇总成的集体。

将这些对象用数字或符号表示(对象映射到数域)，这样集合中的元素就变为了多个数字或符号以某种方式排列成的组合

### 2.1.2 对象的表示

在线代中，由单独的数a构成的元素被称为 `标量` (scalar)：一个标量可以是整数、实数或复数

- $a,b,c$

多个标量 $a_1,a_2,\cdots,a_n$ 按一定顺序组成一个序列，这样的元素称为 `向量`  (vector)

- 向量代表了维度的增加，向量中每个元素需要使用一个索引确定
- $\alpha=(a_1,a_2,\cdots,a_n)$

`矩阵` (matrix) 由维度相同的向量组成

- 矩阵代表了维度的增加，矩阵中每个元素需要使用两个索引确定
- $A=\left(\begin{matrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{matrix}\right)$

`张量` (tensor) 矩阵中的每个元素都是向量，则形成张量

- $A=\left(\begin{matrix}1&1&2&2\\0&1&0&2\\3&3&2&2\\0&3&0&2\end{matrix}\right)=\left(\begin{matrix}1&2\\3&2\end{matrix}\right)\otimes\left(\begin{matrix}1&1\\0&1\end{matrix}\right)$

---

计算机处理离散取值的二进制信息，因此模拟世界的信号必须在定义域和值域上同时进行数字化，才能被计算机存储和处理。

线性代数是用模拟数字世界表示真实物理世界的工具

在计算机存储中：

- `标量` 占据零维度数组
- `向量` 占据一维数组：如语音信号
- `矩阵` 占据二维数组：如灰度图像
- `张量` 占据三维乃至更高维度数组：如RGB图像和视频

### 2.1.3 向量的描述

#### 范数(norm)

> 对单个向量大小的度量，描述向量自身的性质

**将向量映射为一个非负的数值**
$$
L^p范数：\vert X\vert_p=\left(\sum_i\limits\vert x_i\vert^p\right)^{\frac{1}{p}}
$$

##### 常用范数

1-范数：所有元素绝对值的和

2-范数：模长

$\infty$-范数：最大元素的绝对值

#### 内积(inner product)

> 向量之间的关系

$$
(X,Y)=\sum x_i\cdot y_i
$$

内积描述的是 `夹角` 能表示两个向量之间的相对位置

- $正交，X,Y相互垂直\iff (X,Y)=0$ 

  表明两向量相互独立，互不影响

#### 应用

在实际问题中，向量是某些对象或某些行为的特征。范数和内积能处理这些表示特征的数学模型，进而提取出原始对象或原始行为中的隐含关系。

### 2.1.4 向量空间

如果有一个集合，它的元素都是具有相同维数的向量

- 定义了加法和数乘等结构化运算，将这个空间称为 `线性空间` 

- 定义了内积运算的线性空间，将这个空间称为 `内积空间`

**在线性空间中，任一一个向量代表的是n维空间中的一个点；线性空间中的任一点也都可以唯一地用一个向量表示**

#### 线性空间参考系

`正交基` ：一组两两正交的向量构成了这个空间的正交基

`标准正交基` ：正交基中基向量的 $L^2$ 范数的单位长度为1，则这组正交基就是标准正交基

- 正交基的作用就是给内积空间定义基准，用于空间中的所有向量（点）

#### 线性空间上的变化

> 当作为参考系的标准正交基确定后，空间中的点就可以用向量表示，当这个点移动到另一个位置，描述这个点的向量也会发生改变。

`线性变换(linear transformation)` ：如上所述，点的变化对应着向量的线性变换，描述对象变化或向量变换的数学语言是矩阵

##### 变化的实现

点本身变化

- 代表变化的矩阵乘以代表对象的向量

作为参考系的坐标系的变化

- 向量不变，将代表变化的矩阵乘以正交基
- 此时，矩阵的作用相当于对正交基进行变换。

---

如
$$
\begin{aligned}
AX=Y
\end{aligned}
$$

- 向量X经过A描述的变换，变成了同一坐标系下的向量Y
- 一个点在坐标系A下为向量X，在标准坐标系 $\Lambda=\left(\begin{matrix}1&&\\&\ddots&\\&&1\end{matrix}\right)$ 下表示为向量Y

### 2.1.5 矩阵

> 矩阵代表了向量的变化，其效果通常是对原始向量同时施加方向变化和尺度变化

#### 特征值与特征向量的理解

对于一些向量，在变换（矩阵）下，只有伸缩(尺度变化)而没有旋转(方向变化)

这些特殊的向量称为 `特征向量` ，伸缩尺度称为 `特征值`
$$
AX=\lambda X
$$
矩阵特征值和特征向量的意义在于表示了变化的速度和方向

- 特征值表示了变化速度
- 特征向量表示了变化方向

不同特征值和特征向量的叠加是矩阵的效果

#### 特征值分解

> 求解特征值和特征向量的过程称为特征值分解，但矩阵必须是n维方阵

将特征值分解推广到所有矩阵之上，就是更通用的奇异值分解

## 2.2 probablity theory

> 对随机事件发生的可能性进行规范的数学描述是概率论的公理化过程

### 2.2.1 概率与频率

> 频率学派：随着重复试验次数的增加，特定事件出现的频率值就会呈现出稳定性，逐渐趋近于某个常数

`概率` ：一个可独立重复的随机试验中**单个**结果出现频率的极限

#### 依据：古典概率模型

> 试验结果只包含有限个基本事件，且每个基本事件发生的可能性相同。

所有基本事件数目为n，待观察的随机事件A中包含的基本时间数目为k，则古典概率模型下事件概率的计算式为：
$$
P(A)=\frac{k}{n}
$$
**只针对单个随机事件**

### 2.2.2 条件概率

> 用于刻画两个随机事件之间的关系（类比内积，通过运算将关系映射为数值），根据已有信息对样本空间进行调整后得到的新的概率分布

$$
P(A\mid B)=\frac{P(AB)}{P(B)},表示事件A在事件B已经发生的条件下发生的概率
$$

A和B两个事件共同发生的频率称为 `联合概率` ，记为 $P(AB)$

- 如果两个事件发生互不影响相互独立，则其联合概率 $P(AB)=P(A)P(B)$
- 对于相互独立的事件，条件概率就是自身概率 $P(A\mid B)=P(A)$