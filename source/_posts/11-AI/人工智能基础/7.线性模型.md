# 线性模型

## 概述

![image-20230125165532373](7.线性模型/image-20230125165532373.png)

可将 $\theta$ 理解为每个特征的相对权重

![image-20230125165634577](7.线性模型/image-20230125165634577.png)

---

### 线性拟合与线性分类

![image-20230125172132694](7.线性模型/image-20230125172132694.png)

## 梯度下降

###  目标

预测问题——regression prediction

![image-20230124232827304](7.线性模型/image-20230124232827304.png)

假设是线性函数，有 $h_{\theta}=\theta_0+\theta_1 x$ 

求 $h_{\theta}(x^{(0)})=\theta_0+\theta_1 x$ 

### 有监督学习

训练样本 $(x^{(i)},y^{(i)})$ ，$x^{(i)}$ 为第i个样本，$x^{(i)}_j$ 表示第 $i$ 个样本的第j个特征值

![image-20230124234543735](7.线性模型/image-20230124234543735.png)

 目标

![image-20230124234553729](7.线性模型/image-20230124234553729.png)

![image-20230125173726397](7.线性模型/image-20230125173726397.png)

![image-20230124234822706](7.线性模型/image-20230124234822706.png)

![image-20230124234922104](7.线性模型/image-20230124234922104.png)

损失函数：$J(\theta)$  调整参数，使 $J(\theta)$ 最小

构造

- ![image-20230125001548136](7.线性模型/image-20230125001548136.png)

- ![image-20230125001925496](7.线性模型/image-20230125001925496.png)

![image-20230125002004575](7.线性模型/image-20230125002004575.png)

### 梯度下降法

求损失函数的最优值

![image-20230125002346471](7.线性模型/image-20230125002346471.png)

![image-20230125002526728](7.线性模型/image-20230125002526728.png)

![image-20230125002603505](7.线性模型/image-20230125002603505.png)

梯度：控制方向

![image-20230125003230991](7.线性模型/image-20230125003230991.png)

步长由 $\alpha$ 确定

- 步长太大

  ![image-20230125002820820](7.线性模型/image-20230125002820820.png)

- 步长太小

  ![image-20230125002858192](7.线性模型/image-20230125002858192.png)

![image-20230125003342164](7.线性模型/image-20230125003342164.png)

---

![image-20230125003447442](7.线性模型/image-20230125003447442.png)

![image-20230125003545414](7.线性模型/image-20230125003545414.png)

方向对梯度下降影响比步长大，步长决定在梯度方向上走多远

---

![image-20230125004056807](7.线性模型/image-20230125004056807.png)

![image-20230125004247026](7.线性模型/image-20230125004247026.png)

![image-20230125004307107](7.线性模型/image-20230125004307107.png)

![image-20230125004443530](7.线性模型/image-20230125004443530.png)

- 两个参数上步同时更新  

  ![image-20230125004632853](7.线性模型/image-20230125004632853.png)

---

一般化梯度下降算法

![image-20230125085715788](7.线性模型/image-20230125085715788.png)

$\theta_i$ 的大小表示第 i 个特征的重要程度（对结果的影响大小）

![image-20230125090724029](7.线性模型/image-20230125090724029.png)

![image-20230125091241364](7.线性模型/image-20230125091241364.png)

![image-20230125091323136](7.线性模型/image-20230125091323136.png)

### 不同特征尺度不同，需要进行归一化

![image-20230125091826326](7.线性模型/image-20230125091826326.png)

![image-20230125091922644](7.线性模型/image-20230125091922644.png)

### 牛顿方法求方程的解（梯度下降）

![image-20230125092739431](7.线性模型/image-20230125092739431.png)

![image-20230125092756107](7.线性模型/image-20230125092756107.png)

![image-20230125092810048](7.线性模型/image-20230125092810048.png)

### 局限性

适用于严格凸函数，

若存在局部最优，则需要相应的处理方法

- 尽量不构造有局部最优的损失函数
- 多采样，给定不同随机值，找到最好最优点
- 自适应调整步长，跳出局部最优

## 最小二乘法

### 直线距离与垂直距离关系

![image-20230125173742496](7.线性模型/image-20230125173742496.png)

V中距离与 P中距离相差一个常数 $vcos\alpha=s,tan\alpha=k$  

### 线性模型最优估计

给定数据

![image-20230126093934745](7.线性模型/image-20230126093934745.png)

N个数据

![image-20230126094147085](7.线性模型/image-20230126094147085.png)

参数的矩阵表示 

每一行表示一个数据 $d_i$ ，每一列代表特征分量

![image-20230126095251467](7.线性模型/image-20230126095251467.png)

![image-20230126101840990](7.线性模型/image-20230126101840990.png)

**之后要换成 $\hat{y}^{(i)}$** 

![image-20230126095354764](7.线性模型/image-20230126095354764.png)

![image-20230126095428131](7.线性模型/image-20230126095428131.png)

![image-20230126100228254](7.线性模型/image-20230126100228254.png)

可表示为 ![image-20230126100412895](7.线性模型/image-20230126100412895.png)

![image-20230126100752557](7.线性模型/image-20230126100752557.png)

![image-20230126101012965](7.线性模型/image-20230126101012965.png)

![image-20230126101141483](7.线性模型/image-20230126101141483.png)

![image-20230126101545393](7.线性模型/image-20230126101545393.png)

![image-20230126101649207](7.线性模型/image-20230126101649207.png)

![image-20230126102043400](7.线性模型/image-20230126102043400.png)

### LMS与GD对比

LMS计算量来源于计算求逆的计算量

梯度下降存在局部收敛问题，收敛速度满，步长的选取

最优实践，普通线性模型，数据量不超过百万级，可以不用梯度下降

### 线性组合角度理解最小二乘

#### 几何角度

![image-20230126103231193](7.线性模型/image-20230126103231193.png)

#### 线性组合角度

![image-20230126103306470](7.线性模型/image-20230126103306470.png)

![image-20230126103403959](7.线性模型/image-20230126103403959.png)

#### 矩阵角度

![image-20230126103905791](7.线性模型/image-20230126103905791.png)

![image-20230126103937631](7.线性模型/image-20230126103937631.png)

![image-20230126104049239](7.线性模型/image-20230126104049239.png)

![image-20230126104936480](7.线性模型/image-20230126104936480.png)

![image-20230126104546223](7.线性模型/image-20230126104546223.png)

![image-20230126104343143](7.线性模型/image-20230126104343143.png)

![image-20230126104519870](7.线性模型/image-20230126104519870.png)

![image-20230126104624606](7.线性模型/image-20230126104624606.png)

![image-20230126104701478](7.线性模型/image-20230126104701478.png)

### 概率角度理解最小二乘

![image-20230126105123800](7.线性模型/image-20230126105123800.png)

$\epsilon^{(i)}$ 为误差，服从正态分布

![image-20230126105203549](7.线性模型/image-20230126105203549.png)

![image-20230126105224911](7.线性模型/image-20230126105224911.png)

---

极大似然回顾

![image-20230126105324225](7.线性模型/image-20230126105324225.png)

![image-20230126105448086](7.线性模型/image-20230126105448086.png) 

独立同分布

![image-20230126105644430](7.线性模型/image-20230126105644430.png)

![image-20230126105546354](7.线性模型/image-20230126105546354.png)

----

![image-20230126105806523](7.线性模型/image-20230126105806523.png)

![image-20230126105953786](7.线性模型/image-20230126105953786.png)

![image-20230126110214362](7.线性模型/image-20230126110214362.png)

## 逻辑斯蒂回归

将线线性关系转化为二分类问题

### sigmoid函数

> 0-1间的连续函数 

![image-20230126111212592](7.线性模型/image-20230126111212592.png)

### 步骤

#### 1. 通过线性映射，将所有值映射到01之间

![image-20230126111404726](7.线性模型/image-20230126111404726.png)

![image-20230126111534265](7.线性模型/image-20230126111534265.png)

#### 2. 通过对数几率判断相对关系

![image-20230126111610784](7.线性模型/image-20230126111610784.png)

- =1，一样重要
- \> 1，上边重要
- <1，下边重要

![image-20230126111620875](7.线性模型/image-20230126111620875.png)

几率值是一个线性模型

### 应用

语言情感判断

![image-20230126112650340](7.线性模型/image-20230126112650340.png)

将一句话变成向量形式，$x_i$ 表示每个词的词频， $w_i$ 表示每个词的权重

![image-20230126112954944](7.线性模型/image-20230126112954944.png) 表示整句话，![image-20230126113017715](7.线性模型/image-20230126113017715.png)表示整句话的情感

对于正面词，权重大于0

对于中性词，权重趋于0

对于负面词，权重小于0

某个词重要性：在本篇文章大量出现，在其他文章出现较少

term frequency：文档中词频，越大表示越重要

Inverse Document frequency：其他语料库中出现的词频，越小表示其他文档中出现少，当前文档中的词比较重要

- 其他文档中出现的词频求倒

### 损失函数

线性模型的损失函数

![image-20230126171104764](7.线性模型/image-20230126171104764.png)

逻辑斯蒂回归，给出的是属于某一类的概率

![image-20230126171322408](7.线性模型/image-20230126171322408.png)

损失函数：期望是1，而分为1的概率由 P(y=1|x,\theta) 给出，概率误差为：

![image-20230126173604258](7.线性模型/image-20230126173604258.png)

![image-20230126173815530](7.线性模型/image-20230126173815530.png)

\sum 所有数据

\sum 类别数：两类 0 1

指示函数：相等，为1；不相等，为0；只判断正确部分

源于极大似然估计，用log去掉连乘

P(y=j|h(xi)) 表示预测为该类的概率

I*log表示预测正确部分有多少

越大越好，对于损失函数，应该是越小越好

![image-20230126173833388](7.线性模型/image-20230126173833388.png)

### 多分类问题 softmax 

![image-20230126180048864](7.线性模型/image-20230126180048864.png)

![image-20230126180112523](7.线性模型/image-20230126180112523.png)

---

![image-20230126180641586](7.线性模型/image-20230126180641586.png)

## 正则化线性模型

对线性模型参数做更多约定

![image-20230126180953734](7.线性模型/image-20230126180953734.png)

![image-20230126181043362](7.线性模型/image-20230126181043362.png)

![image-20230126181231887](7.线性模型/image-20230126181231887.png)

![image-20230126181524488](7.线性模型/image-20230126181524488.png)

### 不可逆情况——岭优化

对损失函数限制，\lambda 为惩罚项，使 \theta 的值不太大

![image-20230126181828785](7.线性模型/image-20230126181828785.png)

![image-20230126182235725](7.线性模型/image-20230126182235725.png)

条件优化

![image-20230126181858503](7.线性模型/image-20230126181858503.png)

![image-20230126182049347](7.线性模型/image-20230126182049347.png)

![image-20230126182118885](7.线性模型/image-20230126182118885.png)

![image-20230126182403482](7.线性模型/image-20230126182403482.png)

![image-20230126182850462](7.线性模型/image-20230126182850462.png)

### LASSO

![image-20230126183348781](7.线性模型/image-20230126183348781.png)

![image-20230126183620788](7.线性模型/image-20230126183620788.png)

> 通过将权重设为0，去除冗余特征，选择一部分非零特征

![image-20230126184027665](7.线性模型/image-20230126184027665.png)

min L=0 为一个离散的函数，不好优化

![image-20230126184507611](7.线性模型/image-20230126184507611.png)

用L-1范数代替L-0范数，更接近0范数

![image-20230126184657137](7.线性模型/image-20230126184657137.png)

![image-20230126184758374](7.线性模型/image-20230126184758374.png)

通过减少特征维数，减少计算复杂度

#### 坐标轴下降

![image-20230126233031559](7.线性模型/image-20230126233031559.png)

![image-20230126233224116](7.线性模型/image-20230126233224116.png)

![image-20230126233202598](7.线性模型/image-20230126233202598.png)

需要对每个维度都要进行搜索

#### 前向选择

线性组合

![image-20230126233613092](7.线性模型/image-20230126233613092.png)

![image-20230126233746978](7.线性模型/image-20230126233746978.png)

![image-20230126234151061](7.线性模型/image-20230126234151061.png)

**具体计算：**

在一个特征方向上确定最优值，再确定其他方向

![image-20230126234434544](7.线性模型/image-20230126234434544.png)

向右，损失函数减小；再向右，损失函数增大，则拐点为一个方向上最优值

#### 前向梯度

多个特征方向同时调整

每一步考虑所有方向上的最优解

![image-20230126234953434](7.线性模型/image-20230126234953434.png)

每一步都计算相应的损失函数

![image-20230126235105144](7.线性模型/image-20230126235105144.png)

![image-20230126235449485](7.线性模型/image-20230126235449485.png)

#### 最小角回归LARS

![image-20230126235933067](7.线性模型/image-20230126235933067.png)

![image-20230127001723973](7.线性模型/image-20230127001723973.png)

y在平面 x_1,x_2 上的投影，通过对角平分线的修正，找到y在平面上的最佳投影\hat{y}

J(\theta) =J(y-\hat{y})最小
